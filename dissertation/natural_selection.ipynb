{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Selection (Walmart) - SIOP 2019 Competition\n",
    "\n",
    "- Natural Selection = Natural Language Processing + Global Selection and Assessment\n",
    "\n",
    "This competition consisted of a data set containing open-ended resposes to 5 situational judgment items and 5 aggregated personality trait scores. The goal of the competition was to generate the best mean prediction across all 5 traits using only these open-ended responses.\n",
    "\n",
    "We used three approaches:\n",
    "- Key Words: a sample of responses from the high- and low-end of each trait distribution were read and then key words were extracted which seemed to occur more at one end of the distribution than the other\n",
    "- Machine learning: machine learning techniques were used with features from Key Words and other data extracted from the text\n",
    "- Deep learning: deep learning techniques were used. This is the most refined code and the place where experienced data scientists would find most value in reviewing\n",
    "\n",
    "The winning submission resulted from combining the methods.\n",
    "\n",
    "Note on the code contained in this notebook:\n",
    "- We removed most of the exploratory code from this notebook to focus on what we actually used in the final predictions. Some irrelevant and duplicte elements remain. This code was written by different people with different levels of coding expertise. Thus, the application of code can vary widely and may seem disjointed/incoherent at times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "- pandas (https://pandas.pydata.org/)\n",
    "- numpy (http://www.numpy.org/)\n",
    "- seaborn (https://seaborn.pydata.org/)\n",
    "- scikit-learn (https://scikit-learn.org/)\n",
    "- scipy (https://www.scipy.org/)\n",
    "- pyspellchecker (https://github.com/barrust/pyspellchecker)\n",
    "- textblob (https://textblob.readthedocs.io/en/dev/)\n",
    "- spacy (https://spacy.io/)\n",
    "- tpot (http://epistasislab.github.io/tpot/)\n",
    "- xgboost (https://xgboost.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/388779629.py:56: DeprecationWarning: Please import `pearsonr` from the `scipy.stats` namespace; the `scipy.stats.stats` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "# Python's best-known DataFrame implementation\n",
    "import pandas as pd\n",
    "\n",
    "# Fast, flexible array and numerical linear algebra subroutines\n",
    "import numpy as np\n",
    "\n",
    "# OS utilities (e.g. path module)\n",
    "import os\n",
    "\n",
    "# Plots & other visualization\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pretty printing of complex datatypes\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "# Preprocessing and modeling utilities\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Evaluation\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Text processing tools\n",
    "from spellchecker import SpellChecker\n",
    "from textblob import TextBlob\n",
    "\n",
    "# import language_check\n",
    "\n",
    "\n",
    "from textstat.textstat import textstatistics, legacy_round \n",
    "\n",
    "\n",
    "# For word embeddings and syntactic features\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# AutoML\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "#xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Here we set some constant values related to local paths to data files as well as lists containing the various predictor and target features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      "Train    1088\n",
      "Dev       300\n",
      "Test      300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/I745133/Desktop/git/NLP/dissertation/data/MLdata.csv')\n",
    "types = df['Dataset'].value_counts()\n",
    "print(types)\n",
    "train_df = df[df['Dataset'] == 'Train']\n",
    "valid_df = df[df['Dataset'] == 'Dev']\n",
    "test_df = df[df['Dataset'] == 'Test']\n",
    "train_df.to_csv('/Users/I745133/Desktop/git/NLP/dissertation/data/train_rep.csv', index=False)\n",
    "valid_df.to_csv('/Users/I745133/Desktop/git/NLP/dissertation/data/valid_rep.csv', index=False)\n",
    "test_df.to_csv('/Users/I745133/Desktop/git/NLP/dissertation/data/test_rep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to various data targets for the competition. \n",
    "\n",
    "# Update to reflect the directory hierarchy of your machine \n",
    "DATA_DIR = \"/Users/I745133/Desktop/git/NLP/dissertation/data\"\n",
    "\n",
    "TRAIN_CSV_DATA_NAME = \"/Users/I745133/Desktop/git/NLP/dissertation/data/train_rep.csv\"\n",
    "TEST_CSV_DATA_NAME = \"/Users/I745133/Desktop/git/NLP/dissertation/data/valid_rep.csv\"\n",
    "FINAL_CSV_DATA_NAME = \"/Users/I745133/Desktop/git/NLP/dissertation/data/test_rep.csv\"\n",
    "\n",
    "# Set some DataFrame-specific constants\n",
    "TARGET_COLUMN_NAMES = [attribute + \"_Scale_score\" for attribute in [\"A\", \"E\", \"O\", \"N\", \"C\"]]\n",
    "PREDICTOR_TEXT_COLUMN_NAMES = [\"open_ended_\" + str(idx) for idx in range(1, 6)]\n",
    "PREDICTOR_CONCAT_COLUMN_NAME = \"open_ended_6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In Data\n",
    "\n",
    "Here we use `pandas` to read our csv data sets into a DataFrame, a common and convenient data structure for the workflows we will be implementing. `df_train` will be used for training purposes, `df_test` will be used for public leaderboard submissions, and 'df_train' will be used for the private leaderboards submissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source\n",
       "Train    1088\n",
       "Test      300\n",
       "Final     300\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv data to base DataFrame\n",
    "df_train_temp = pd.read_csv(TRAIN_CSV_DATA_NAME)\n",
    "df_test_temp = pd.read_csv(TEST_CSV_DATA_NAME)\n",
    "df_final_temp = pd.read_csv(FINAL_CSV_DATA_NAME)\n",
    "\n",
    "df_train_temp['Source']='Train'\n",
    "df_test_temp['Source']='Test'\n",
    "df_final_temp['Source']='Final'\n",
    "\n",
    "# Combine datasets datasets\n",
    "df_total=pd.concat([df_train_temp,df_test_temp,df_final_temp],ignore_index=True, sort=True)\n",
    "\n",
    "# Check data load\n",
    "df_total['Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Respondent_ID</th>\n",
       "      <th>open_ended_1</th>\n",
       "      <th>open_ended_2</th>\n",
       "      <th>open_ended_3</th>\n",
       "      <th>open_ended_4</th>\n",
       "      <th>open_ended_5</th>\n",
       "      <th>E_Scale_score</th>\n",
       "      <th>A_Scale_score</th>\n",
       "      <th>O_Scale_score</th>\n",
       "      <th>C_Scale_score</th>\n",
       "      <th>N_Scale_score</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10446116527</td>\n",
       "      <td>I would change my vacation week, because I am ...</td>\n",
       "      <td>I would reach out to my boss and ask him or he...</td>\n",
       "      <td>I would not go. I am a not a social person. I ...</td>\n",
       "      <td>I would ask my manager why he/she gave me such...</td>\n",
       "      <td>I would find this experience super enjoyable. ...</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>Train</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10440100535</td>\n",
       "      <td>I would talk to my colleague and see if they w...</td>\n",
       "      <td>I would continue to work on the project that w...</td>\n",
       "      <td>I would talk to my colleague and try to talk t...</td>\n",
       "      <td>I would feel upset about the negative feedback...</td>\n",
       "      <td>I would find this experience enjoyable. I feel...</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>Train</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10462850071</td>\n",
       "      <td>I would feel upset because perhaps I already b...</td>\n",
       "      <td>I would start working on the project now and g...</td>\n",
       "      <td>I would feel guilty about thinking about not g...</td>\n",
       "      <td>I would feel really defensive about it. I woul...</td>\n",
       "      <td>I would find it enjoyable because I would be r...</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>Train</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10460008027</td>\n",
       "      <td>I would suggest that whoever requested the tim...</td>\n",
       "      <td>I would try to finish early because it's alway...</td>\n",
       "      <td>I wouldn't want to go, but I'd have to weigh t...</td>\n",
       "      <td>I would first wait until I'm calm.  Then I wou...</td>\n",
       "      <td>I would love to have the opportunity to learn ...</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>Train</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10459746373</td>\n",
       "      <td>I would talk to my colleague to see if he has ...</td>\n",
       "      <td>I would remind my boss that I am working on th...</td>\n",
       "      <td>I would go anyway. Networking is a good way to...</td>\n",
       "      <td>I would talk to my manager first and get some ...</td>\n",
       "      <td>I would find this experience enjoyable as I am...</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>Train</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Respondent_ID                                       open_ended_1  \\\n",
       "0    10446116527  I would change my vacation week, because I am ...   \n",
       "1    10440100535  I would talk to my colleague and see if they w...   \n",
       "2    10462850071  I would feel upset because perhaps I already b...   \n",
       "3    10460008027  I would suggest that whoever requested the tim...   \n",
       "4    10459746373  I would talk to my colleague to see if he has ...   \n",
       "\n",
       "                                        open_ended_2  \\\n",
       "0  I would reach out to my boss and ask him or he...   \n",
       "1  I would continue to work on the project that w...   \n",
       "2  I would start working on the project now and g...   \n",
       "3  I would try to finish early because it's alway...   \n",
       "4  I would remind my boss that I am working on th...   \n",
       "\n",
       "                                        open_ended_3  \\\n",
       "0  I would not go. I am a not a social person. I ...   \n",
       "1  I would talk to my colleague and try to talk t...   \n",
       "2  I would feel guilty about thinking about not g...   \n",
       "3  I wouldn't want to go, but I'd have to weigh t...   \n",
       "4  I would go anyway. Networking is a good way to...   \n",
       "\n",
       "                                        open_ended_4  \\\n",
       "0  I would ask my manager why he/she gave me such...   \n",
       "1  I would feel upset about the negative feedback...   \n",
       "2  I would feel really defensive about it. I woul...   \n",
       "3  I would first wait until I'm calm.  Then I wou...   \n",
       "4  I would talk to my manager first and get some ...   \n",
       "\n",
       "                                        open_ended_5  E_Scale_score  \\\n",
       "0  I would find this experience super enjoyable. ...       2.250000   \n",
       "1  I would find this experience enjoyable. I feel...       4.666667   \n",
       "2  I would find it enjoyable because I would be r...       2.250000   \n",
       "3  I would love to have the opportunity to learn ...       2.916667   \n",
       "4  I would find this experience enjoyable as I am...       3.750000   \n",
       "\n",
       "   A_Scale_score  O_Scale_score  C_Scale_score  N_Scale_score Dataset Source  \n",
       "0       3.750000       3.166667       3.750000       2.916667   Train  Train  \n",
       "1       4.416667       4.583333       5.000000       1.333333   Train  Train  \n",
       "2       4.750000       4.083333       4.666667       2.166667   Train  Train  \n",
       "3       4.083333       3.916667       4.916667       1.333333   Train  Train  \n",
       "4       4.750000       3.666667       4.916667       1.583333   Train  Train  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Modules\n",
    "\n",
    "Here we define various preprocessing utilities (simple python functions that operate on a single input) as well as preprocessing transformers which operate on an entire column of data. Transformers should be implemented as Python classes that inherit from `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin` & should implement a `fit` and `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[PREDICTOR_CONCAT_COLUMN_NAME] = df_total.apply(\n",
    "    lambda row: \" \".join([row[col_name] for col_name in PREDICTOR_TEXT_COLUMN_NAMES]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "PREDICTOR_TEXT_COLUMN_NAMES_ALL =['open_ended_1','open_ended_2','open_ended_3',\n",
    "                                  'open_ended_4','open_ended_5','open_ended_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and Correct Spelling Errors. \n",
    "\n",
    "spell_checker = SpellChecker()\n",
    "\n",
    "def tokenize(text):\n",
    "    return TextBlob(text).words\n",
    "\n",
    "def compute_num_spelling_errors(text):\n",
    "    return len(spell_checker.unknown(tokenize(text)))\n",
    "\n",
    "def divide(x, y):\n",
    "    return x / y\n",
    "\n",
    "def word_count(text): \n",
    "    return textstatistics().lexicon_count(text, removepunct=True)\n",
    "\n",
    "for predictor_col in PREDICTOR_TEXT_COLUMN_NAMES_ALL:\n",
    "    df_total[predictor_col + \"_num_words\"] = df_total[predictor_col].apply(word_count)\n",
    "    df_total[predictor_col + \"_num_misspelled\"] = df_total[predictor_col].apply(compute_num_spelling_errors)\n",
    "    df_total[predictor_col + \"_percent_misspelled\"] = df_total[[predictor_col + \"_num_misspelled\",\n",
    "                              predictor_col + \"_num_words\"\n",
    "    ]].apply(lambda x: divide(*x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building word lists 1\n",
    "\n",
    "We build the word lists twice because we were lazy. The lists diverged due to different team members refining them and we never got around to reconciling the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists were compliled by reading a sample of comments at either the top or bottom 5% of each trait distribution\n",
    "\n",
    "O_high_5_LIST = [\"accept\",\"allow\",\"apply\",\"benefit\",\"better\",\"career\",\"client\",\"comfortable\",\"contact\",\"contribute\",\"convince\",\n",
    "\"correct\",\"enjoy\",\"excited\",\"fair\",\"first\",\"fun\",\"great time\",\"grow\",\"happy to go\",\"help\",\"immediately\",\"improve\",\"insist\",\n",
    "\"leader\",\"learn\",\"let\",\"mad\",\"negative\",\"no problem\",\"offer\",\"personal issue\",\"respect\",\"right away\",\"show\",\"team\"]\n",
    "\n",
    "C_high_2_LIST = [\"family\",\"report\",\"stress\",\"question\",\"convince\",\"job\",\"deserve\",\"longest\",\"comfortable\",\"win\",\"great time\",\n",
    "\"negative\",\"fair\",\"check in\",\"short time\",\"accus\",\"short \",\"respect\",\"willing\",\"lie\",\"correct\",\"as soon\",\"positive\",\"impres\",\n",
    "\"review\",\"problems\",\"immediately\",\"hate networking\",\"anger\",\"proof\",\"upset\",\"prove\",\"open\",\"explain\",\"improve\",\"time \",\n",
    "\"confident\",\"right away\",\"let\"]\n",
    "\n",
    "A_high_1_LIST = [\"agree\",\"benefit\",\"best\",\"bonus\",\"change\",\"compromise\",\"considerate\",\"correct\",\"defer\",\"easy going\",\"family\",\n",
    "\"flexible\",\"fun\",\"good\",\"help\",\"hurt\",\"incorrect\",\"leader\",\"let\",\"misunderstand\",\"no problem\",\"not interested\",\"obligation\",\n",
    "\"paid\",\"pick\",\"priority\",\"problems\",\"quickly\",\"respect\",\"review\",\"show\",\"willing\",\"win\"]\n",
    "\n",
    "E_high_3_LIST = [\"career\",\"good\",\"frustrated\",\"nice\",\"best\",\"deny\",\"reflect\",\"confident\",\"grow\",\"consequence\",\"missed out\",\n",
    "\"connect\",\"rage\",\"importan\",\"worry\",\"I am sociable\",\"party\",\"right away\",\"priority\",\"sociable\",\"accept\",\"focus\",\"plan \",\n",
    "\"report\",\"excited\",\"reward\",\"contribute\",\"allow\",\"success\",\"contact\",\"review\",\"absolutely go\",\"for sure go\",\"meet\",\"great\",\n",
    "\"colleagues\",\"social\",\"not need anyone\",\"regardless\",\"fool\",\"surely attend\",\"leader\",\"network\",\"I like parties\",\"no problem\",\n",
    "\"learn\",\"friendship\",\"definitely go\",\"introduce\",\"let\",\"competition\",\"client\", \"make new friends\"]\n",
    "\n",
    "A_low_2_LIST = [\"wrong\",\"question\",\"busy\",\"probably\",\"resent\",\"not go\",\"importan\",\"fun\",\"enjoy\",\"bad\",\"first\",\"problems\",\n",
    "\"refuse\",\"better\",\"short time\",\"good\",\"anxiety\",\"avoid going\",\"respect\",\"compromise\",\"losing\",\"angry\",\"regardless\",\n",
    "\"social anxiety\",\"rage\",\"decline\",\"pretend\",\"focus\",\"connect\",\"no problem\",\"priority\",\"excuse\",\"procrastinate\",\"fool\",\"sick\",\n",
    "\"personal issue\",\"anticipat\",\"deadline\",\"anyone\",\"lose\",\"difficult\",\"meet\",\"judg\",\"worry\",\"plan \",\"trouble\",\"show\",\"nervous\",\n",
    "\"reflect\",\"help\",\"pressure\",\"compensate\",\"bonus\",\"get along\",\"flexible\",\"colleagues\",\"accus\",\"fire\",\"consequence\",\"demand\",\n",
    "\"not back down\",\"stand my ground\"]\n",
    "            \n",
    "A_low_3_LIST = [\"I like parties\",\"fire\",\"unpleasant\",\"would not go\",\"sales\",\"quit\",\"discomfort\",\"money\",\"hate networking\",\n",
    "\"worthwhile\",\"obligation\",\"panic\",\"emotion\",\"unlikely\",\"hell\",\"skip\",\"social anxious\",\"pick\",\"cold\",\"decline\",\"not go\",\"paid\",\n",
    "\"get out of it\",\"hate\",\"wouldn't go\",\"reward\",\"rage\",\"short \",\"negotiate\",\"beg\",\"difficult\",\"trouble\",\"resent\",\"time \",\n",
    "\"immediately\",\"stress\",\"stressed\",\"stressed out\",\"reconsider\",\"short time\",\"grow\",\"extremely uncomfortable\",\"willing\",\n",
    "\"get along\",\"apply\",\"if i had to\",\"risk\",\"anxiety\",\"great\",\"forc\",\"allow\",\"socially awkward\",\"dislike\",\"I am sociable\",\n",
    "\"great time\",\"missed out\",\"compensate\",\"oppurtunity\",\"anger\",\"benefit\",\"plan \",\"confirm\",\"avoid\",\"social anxiety\",\"fair\",\n",
    "\"pressure\",\"mad\",\"deserve\",\"not a social person\"]\n",
    "A_low_4_LIST = [\"rage\",\"cold\",\"fool\",\"marked\",\"depend\",\"demand\",\"quit\",\"report\",\"probably\",\"career\",\"accept\",\"not go\",\n",
    "\"compensate\",\"pressure\",\"quiet\",\"angry\",\"afraid\",\"confront\",\"emotion\",\"job\",\"benefit\",\"mad\",\"threaten\",\"money\",\"unpleasant\",\n",
    "\"anxiety\",\"pissed\",\"anyone\",\"obligation\",\"confident\",\"short \",\"regardless\",\"refuse\",\"appeal\",\"hesitate\",\"examples\",\"immediately\",\n",
    "\"bad\",\"suck it up\",\"resent\",\"respect\",\"wrong\",\"harm\"]\n",
    "A_low_5_LIST = [\"paid\",\"refuse\",\"avoid going\",\"alone\",\"emotion\",\"pretend\",\"resent\",\"bonus\",\"win\",\"rage\",\"difficult\",\"probably\",\n",
    "\"afraid\",\"anger\",\"forc\",\"hate networking\",\"change\",\"agree\",\"depend\",\"wouldn't go\",\"pick\",\"focus\",\"obligation\",\"frustrated\",\n",
    "\"considerate\",\"right away\",\"time \",\"money\",\"negative\",\"colleagues\",\"awkward\",\"improve\",\"success\",\"explain\",\"bad\",\"best\",\n",
    "\"respect\",\"let\",\"better\",\"nice\",\"nervous\"]\n",
    "\n",
    "N_low_1_LIST = [\"as soon\",\"report\",\"show\",\"problems\",\"best\",\"quickly\",\"bonus\",\"tense\",\"social\",\"correct\",\"win\",\"concede\",\n",
    "\"leader\",\"misunderstand\",\"unlikely\",\"incorrect\",\"fire\",\"easy going\",\"paid\",\"hesitate\",\"human resources\",\"time \",\"emotion\",\n",
    "\"worried\",\"racist\",\"slash\",\"fun\",\"valid\",\"stubborn\",\"flexible\",\"review\",\"beg\",\"respect\",\"benefit\",\"open\",\"threaten\",\"short \",\n",
    "\"change\",\"first\",\"trouble\",\"agree\",\"compromise\",\"defend\",\"defer\",\"mad\",\"harm\",\"worry\"]\n",
    "N_low_2_LIST = [\"hate networking\",\"client\",\"responsible\",\"longest\",\"unhappy\",\"willing\",\"accus\",\"proof\",\"difficult\",\"family\",\n",
    "\"anger\",\"team\",\"correct\",\"consequence\",\"comfortable\",\"stick\",\"trouble\",\"job\",\"pressure\",\"benefit\",\"mad\",\"report\",\"deserve\",\n",
    "\"accept\",\"positive\",\"review\",\"open\",\"as soon\",\"risk\",\"time \",\"let\",\"feel pressure\",\"check in\",\"depend\",\"dislike\",\"judg\",\n",
    "\"social anxiety\",\"resent\",\"lie\",\"explain\",\"upset\",\"hard \",\"leader\",\"frustrated\"]\n",
    "N_low_3_LIST = [\"learn\",\"no problem\",\"regardless\",\"network\",\"good\",\"introduce\",\"anyone\",\"definitely go\",\"confident\",\"meet\",\n",
    "\"competition\",\"contact\",\"lie\",\"client\",\"I like parties\",\"not need anyone\",\"great\",\"social\",\"party\",\"worry\",\"friendship\",\n",
    "\"review\",\"contribute\",\"stretch myself\",\"surely attend\",\"fool\",\"plan \",\"help\",\"leader\",\"missed out\",\"for sure go\",\"fair\",\n",
    "\"let\",\"reluctance\",\"absolutely go\",\"excited\",\"happy to go\",\"priority\",\"excuse\",\"hard \",\"report\",\"job\",\"anger\"]\n",
    "N_low_5_LIST = [\"learn\",\"anyone\",\"short time\",\"help\",\"leader\",\"client\",\"great time\",\"enjoy\",\"importan\",\"excited\",\"hesitate\",\n",
    "\"correct\",\"lie\",\"team\",\"losing\",\"career\",\"responsible\",\"insist\",\"immediately\",\"bad\",\"happy to go\",\"pretend\",\"willing\",\n",
    "\"emotion\",\"short \",\"stress\",\"confus\",\"trouble\",\"time \",\"worry\",\"success\",\"regardless\",\"report\",\"hurt\",\"show\",\"money\",\n",
    "\"contact\",\"stick\",\"mad\",\"unlikely\"]\n",
    "\n",
    "C_high_1_LIST = [\"question\",\"stubborn\",\"would change\",\"reconsider\",\"as soon\",\"human resources\",\"disagree\",\"defer\",\"risk\",\n",
    "\"unpleasant\",\"immediately\",\"worry\",\"argue\",\"petty\",\"explain\",\"mad\",\"proof\",\"hurt\",\"correct\",\"obligated\",\"not go\",\"harm\",\n",
    "\"unhappy\",\"leader\",\"misunderstand\",\"win\",\"fire\",\"unlikely\",\"first\",\"pick\",\"angry\",\"priority\",\"bonus\",\"quickly\",\"short time\",\n",
    "\"hesitate\",\"tense\",\"social\",\"switch\",\"success\",\"problems\",\"not interested\",\"easy going\",\"no problem\",\"report\",\"reflect\",\n",
    "\"upset\",\"anger\",\"team\",\"valid\",\"paid\",\"review\",\"agree\",\"short \",\"willing\",\"fun\",\"concede\",\"show\",\"seniority\",\"flexible\",\n",
    "\"change\"]\n",
    "C_high_3_LIST = [\"no problem\",\"introvert\",\"frustrated\",\"win\",\"sad\",\"missed out\",\"alone\",\"dislike\",\"appeal\",\"depend\",\"insist\",\n",
    "\"sick\",\"success\",\"not comfortable\",\"stretch myself\",\"report\",\"benefit\",\"accept\",\"hard \",\"contribute\",\"responsible\",\"compensate\",\n",
    "\"fool\",\"social\",\"absolutely go\",\"regardless\",\"anyone\",\"focus\",\"pretend\",\"worried\",\"nightmare\",\"not need anyone\",\"surely attend\",\n",
    "\"for sure go\",\"colleagues\",\"competition\",\"let\",\"help\",\"best\",\"great\",\"deny\",\"importan\",\"learn\",\"network\",\"client\",\"uncomfortable\",\n",
    "\"priority\",\"lie\",\"improve\",\"good\",\"not attend\",\"fun\",\"definitely go\",\"mad\",\"comfortable\",\"reluctant\",\"excited\",\"excuse\",\"meet\"]\n",
    "C_high_4_LIST = [\"willing\",\"change\",\"respect\",\"connect\",\"fun\",\"paid\",\"hard \",\"immediately\",\"terrible\",\"grow\",\"incorrect\",\"refuse\",\n",
    "\"open\",\"resent\",\"quickly\",\"contact\",\"calm\",\"party\",\"short \",\"contribute\",\"my right\",\"stubborn\",\"rebut\",\"problems\",\"worried\",\n",
    "\"as soon\",\"compromise\",\"hurt\",\"good\",\"proof\",\"not true\",\"early\",\"human resources\"\"obligation\",\"colleagues\",\"meet\",\"demand\",\n",
    "\"success\",\"negative\",\"allow\",\"concerned\",\"disagree\",\"let\",\"agree\"]\n",
    "C_high_5_LIST = [\"success\",\"appeal\",\"worry\",\"fun\",\"busy\",\"hesitate\",\"problems\",\"allow\",\"hurt\",\"improve\",\"excited\",\"good\",\"bad\",\n",
    "\"leader\",\"stress\",\"importan\",\"excuse\",\"introduce\",\"lose\",\"enjoy\",\"prove\",\"personal issue\",\"fair\",\"quickly\",\"correct\",\"stick\",\n",
    "\"accus\",\"unlikely\",\"comfortable\",\"sad\",\"willing\",\"contact\",\"confus\",\"career\",\"show\",\"losing\",\"immediately\",\"compensate\",\n",
    "\"anyone\",\"lie\",\"client\",\"help\",\"learn\"]\n",
    "\n",
    "A_high_2_LIST = [\"agree\",\"negative\",\"benefit\",\"overwhelmed\",\"quiet\",\"I had to\",\"lie\",\"team\",\"check in\",\"early\",\"stick\",\n",
    "\"feel pressure\",\"allow\",\"family\",\"sacrifice\",\"stressed\",\"learn\",\"frustrated\",\"right away\",\"convince\",\"best\",\"let\",\"fair\",\n",
    "\"client\",\"longest\",\"responsible\",\"mad\",\"stressed out\",\"report\",\"time \",\"upset\",\"confident\",\"dislike\",\"unhappy\",\"anger\",\n",
    "\"explain\",\"positive\",\"stress\",\"proof\",\"avoid confrontation\",\"more than willing\",\"don't want conflict\",\"easy going\",\n",
    "\"hate conflict\",\"keep people happy\",\"team player\"]\n",
    "A_high_3_LIST = [\"change\",\"reluctant\",\"angry\",\"quickly\",\"right away\",\"excuse\",\"stick\",\"would change\",\"early\",\"compromise\",\n",
    "\"not comfortable\",\"learn\",\"positive\",\"avoid going\",\"anxious\",\"colleagues\",\"fool\",\"reluctance\",\"absolutely go\",\"fun\",\n",
    "\"not attend\",\"tired\",\"losing\",\"worry\",\"busy\",\"no problem\",\"contribute\",\"explain\",\"hurt\",\"network\",\"uncomfortable\",\"consequence\",\n",
    "\"social\",\"not need anyone\",\"surely attend\",\"regardless\",\"help\",\"better\",\"excited\",\"importan\",\"priority\",\"responsible\",\n",
    "\"outside of my comfort zone\",\"party\",\"stretch myself\",\"for sure go\",\"hard \",\"report\",\"focus\",\"client\",\"alone\",\"lie\",\"introduce\",\n",
    "\"friendship\",\"comfortable\",\"contact\",\"best\",\"good\",\"definitely go\",\"anyone\",\"meet\"]\n",
    "A_high_4_LIST = [\"convince\",\"defend\",\"lose\",\"accus\",\"worthwhile\",\"agitated\",\"personal\",\"consequence\",\"concerned\",\"impres\",\n",
    "\"anger\",\"success\",\"correct\",\"win\",\"confus\",\"argue\",\"proof\",\"incorrect\",\"focus\",\"terrible\",\"best\",\"negative\",\"not justified\",\n",
    "\"as soon\",\"plead\",\"confirm\",\"lie\",\"unfair\",\"early\",\"judg\",\"stressed out\",\"hard \",\"organize\",\"risk\",\"improve\",\"worried\",\"quickly\",\n",
    "\"my right\",\"open\",\"frustrated\",\"contact\",\"meet\",\"compromise\",\"pretend\",\"rebut\",\"stress\",\"reconsider\",\"hurt\",\"would not go\",\n",
    "\"importan\",\"positive\",\"problems\",\"agree\",\"let\",\"negotiate\",\"allow\",\"explain\",\"learn\",\"prove\",\"better\",\"anxious\",\"colleagues\",\n",
    "\"not true\",\"upset\",\"grow\"]\n",
    "A_high_5_LIST = [\"accept\",\"short time\",\"question\",\"happy to go\",\"excited\",\"hard \",\"impres\",\"good\",\"grow\",\"losing\",\"reward\",\n",
    "\"show\",\"contribute\",\"convince\",\"accus\",\"willing\",\"concerned\",\"dislike\",\"contact\",\"hesitate\",\"network\",\"comfortable\",\"apply\",\n",
    "\"leader\",\"immediately\",\"stress\",\"correct\",\"importan\",\"great time\",\"hurt\",\"offer\",\"confus\",\"help\",\"anyone\",\"lie\",\"client\",\n",
    "\"enjoy\",\"learn\"]\n",
    "\n",
    "N_high_1_LIST = [\"willing\",\"regardless\",\"losing\",\"great\",\"shy\",\"career\",\"obligated\",\"organize\",\"stick\",\"forc\",\"appeal\",\"anger\",\n",
    "\"unfair\",\"positive\",\"early\",\"reward\",\"my right\",\"I had to\",\"refuse\",\"money\",\"negotiate\",\"personal issue\",\"wrong\",\"anyone\",\n",
    "\"family\",\"enjoy\",\"pissed\",\"hard \",\"team\",\"deny\",\"insist\",\"busy\",\"sacrifice\",\"skip\",\"proof\",\"fair\",\"client\",\"better\",\"contact\",\n",
    "\"meet\",\"question\",\"fool\",\"get even\",\"profanity\",\"cold\",\"unhappy\",\"angry\",\"call in\",\"awkward\",\"excuse\",\"upset\",\"get along\",\n",
    "\"demand\",\"lose\",\"avoid\",\"deadline\",\"stressed\",\"unpleasant\",\"terrible\",\"difficult\",\"frustrated\",\"confront\",\"hell\",\"plead\",\n",
    "\"alone\",\"improve\",\"stare\",\"concerned\",\"hardship\",\"nice\",\"pressure\",\"sad\",\"reflect\",\"probably\",\"friendship\",\"reluctant\",\n",
    "\"sick\",\"obligation\",\"quit\",\"hate\",\"offer\",\"hard stance\"]\n",
    "N_high_2_LIST = [\"calm\",\"panic\",\"stress\",\"enjoy\",\"bonus\",\"show\",\"learn\",\"question\",\"decline\",\"sick\",\"importan\",\"colleagues\",\n",
    "\"worried\",\"worry\",\"connect\",\"meet\",\"rage\",\"paid\",\"pretend\",\"anxiety\",\"avoid going\",\"lose\",\"early\",\"bad\",\"angry\",\"better\",\n",
    "\"deadline\",\"losing\",\"hurt\",\"priority\",\"no problem\",\"wrong\",\"demand\",\"beg\",\"I had to\",\"busy\",\"compromise\",\"negotiate\",\"probably\"]\n",
    "N_high_3_LIST = [\"wrong\",\"compromise\",\"respect\",\"risk\",\"show\",\"afraid\",\"bonus\",\"worried\",\"tense\",\"worthwhile\",\"dislike\",\"valid\",\n",
    "\"confirm\",\"socially awkward\",\"introvert\",\"losing\",\"deserve\",\"quickly\",\"beg\",\"plead\",\"mad\",\"change\",\"better\",\"angry\",\"apply\",\n",
    "\"not comfortable\",\"lose\",\"get out of it\",\"agree\",\"paid\",\"outside of my comfort zone\",\"not a social person\",\"miss out\",\n",
    "\"time \",\"family\",\"reconsider\",\"anxious\",\"short time\",\"prove\",\"negative\",\"money\",\"fire\",\"tired\",\"negotiate\",\"I had to\",\"harm\",\n",
    "\"appeal\",\"sacrifice\",\"hell\",\"stress\",\"awkward\",\"forc\",\"hesitate\",\"pressure\",\"trouble\",\"willing\",\"deadline\",\"short \",\"suck it up\",\n",
    "\"get along\",\"loner\",\"stressed\",\"resent\",\"skip\",\"social anxiety\",\"bad\",\"not great at networking\",\"nightmare\",\"shy\",\"avoid\",\n",
    "\"impres\",\"concerned\",\"difficult\",\"probably\",\"compensate\",\"emotion\",\"unpleasant\",\"obligation\",\"nervous\",\"feel pressure\",\n",
    "\"extremely uncomfortable\",\"nerve-wracking\",\"hate networking\",\"immediately\",\"hate\",\"would not go\",\"social anxious\",\"panic\",\n",
    "\"unlikely\",\"discomfort\",\"not go\",\"anxiety\"]\n",
    "N_high_5_LIST = [\"negative\",\"allow\",\"best\",\"hate networking\",\"let\",\"positive\",\"apply\",\"anger\",\"beg\",\"bonus\",\"comfortable\",\n",
    "\"dislike\",\"oppurtunity\",\"obligation\",\"improve\",\"concerned\",\"pick\",\"open\",\"right away\",\"job\",\"rage\",\"probably\",\"refuse\",\n",
    "\"upset\",\"afraid\",\"risk\",\"alone\",\"social anxiety\",\"consequence\",\"agree\",\"prove\",\"fair\",\"colleagues\",\"awkward\",\"paid\",\"grow\",\n",
    "\"avoid going\",\"early\",\"nervous\",\"forc\",\"depend\",\"resent\",\"frustrated\",\"difficult\"]\n",
    "\n",
    "O_high_1_LIST = [\"accept\",\"anyone\",\"as soon\",\"best\",\"bonus\",\"defer\",\"easy going\",\"enjoy\",\"excuse\",\"flexible\",\"fool\",\"get even\",\n",
    "\"good\",\"harm\",\"hesitate\",\"hurt\",\"importan\",\"leader\",\"marked\",\"meet\",\"negotiate\",\"obligation\",\"paid\",\"petty\",\"pick\",\"plead\",\n",
    "\"positive\",\"probably\",\"problems\",\"quickly\",\"quit\",\"reflect\",\"respect\",\"reward\",\"short \",\"short time\",\"stubborn\",\"suck it up\",\n",
    "\"suffer\",\"tense\",\"terrible\",\"threaten\",\"time \",\"upset\",\"willing\",\"win\",\"worried\"]\n",
    "O_high_2_LIST = [\"agree\",\"allow\",\"anger\",\"best\",\"better\",\"calm\",\"correct\",\"deserve\",\"difficult\",\"excuse\",\"explain\",\"fair\",\n",
    "\"forc\",\"frustrated\",\"fun\",\"great time\",\"immediately\",\"importan\",\"improve\",\"learn\",\"let\",\"nervous\",\"offer\",\"pick\",\"positive\",\n",
    "\"pressure\",\"problems\",\"proof\",\"prove\",\"respect\",\"responsible\",\"review\",\"short \",\"short time\",\"show\",\"suffer\",\"team\",\"time \",\n",
    "\"trouble\",\"upset\",\"worried\"]\n",
    "O_high_3_LIST = [\"absolutely go\",\"accept\",\"alone\",\"angry\",\"anyone\",\"better\",\"career\",\"client\",\"cold\",\"comfortable\",\"consequence\",\n",
    "\"contact\",\"contribute\",\"definitely go\",\"deny\",\"depend\",\"difficult\",\"early\",\"emotion\",\"excited\",\"excuse\",\"feel pressure\",\"focus\",\n",
    "\"for sure go\",\"forc\",\"friendship\",\"fun\",\"good\",\"hesitate\",\"I like parties\",\"insist\",\"introduce\",\"let\",\"lie\",\"lose\",\"losing\",\n",
    "\"meet\",\"miss out\",\"missed out\",\"money\",\"nerve-wracking\",\"nervous\",\"network\",\"nice\",\"no problem\",\"not comfortable\",\"not need anyone\",\n",
    "\"obligated\",\"oppurtunity\",\"outside of my comfort zone\",\"party\",\"plan \",\"positive\",\"priority\",\"quickly\",\"regardless\",\"responsible\",\n",
    "\"success\",\"trouble\",\"worried\",\"worry\",\"would change\"]\n",
    "O_high_4_LIST = [\"allow\",\"anxious\",\"as soon\",\"benefit\",\"best\",\"better\",\"bonus\",\"client\",\"cold\",\"colleagues\",\"comfortable\",\n",
    "\"concerned\",\"confirm\",\"connect\",\"consequence\",\"deserve\",\"early\",\"explain\",\"fool\",\"forc\",\"fun\",\"great time\",\"grow\",\"help\",\n",
    "\"importan\",\"impres\",\"improve\",\"judg\",\"learn\",\"let\",\"lie\",\"losing\",\"marked\",\"meet\",\"negotiate\",\"nervous\",\"nice\",\"not justified\",\n",
    "\"not true\",\"offer\",\"paid\",\"party\",\"personal\",\"personal issue\",\"plan \",\"positive\",\"pretend\",\"problems\",\"prove\",\"quiet\",\n",
    "\"reconsider\",\"resent\",\"respect\",\"review\",\"risk\",\"stick\",\"stubborn\",\"team\",\"threaten\",\"trouble\"]\n",
    "\n",
    "A_low_1_LIST = [\"stare\",\"responsible\",\"fool\",\"get even\",\"profanity\",\"call in\",\"sick\",\"refuse\",\"emotion\",\"hard stance\",\"racist\",\n",
    "\"slash\",\"hardship\",\"demand\",\"compensate\",\"first\",\"stick\",\"quit\",\"personal issue\",\"excuse\",\"trouble\",\"deny\",\"hell\",\"depend\",\n",
    "\"money\",\"cold\",\"hard \",\"marked\",\"pissed\",\"client\",\"deserve\",\"unfair\",\"fair\",\"resent\",\"reconsider\",\"offer\",\"my right\",\"hate\",\n",
    "\"forc\",\"worry\",\"reward\",\"reluctant\",\"concerned\",\"organize\",\"sad\",\"losing\",\"rage\",\"bad\",\"insist\",\"busy\",\"difficult\",\"appeal\",\n",
    "\"stressed out\",\"stressed\",\"wrong\",\"early\",\"longest\",\"proof\",\"better\",\"petty\",\"improve\",\"contact\",\"avoid\",\"accept\",\"entitle\",\n",
    "\"meet\",\"if i had to\",\"seniority\",\"suffer\",\"comfortable\",\"regardless\",\"personal\"]\n",
    "\n",
    "E_low_3_LIST = [\"social anxiety\",\"extremely uncomfortable\",\"nervous\",\"social anxious\",\"panic\",\"unlikely\",\"impres\",\"anxiety\",\n",
    "\"probably\",\"introvert\",\"immediately\",\"feel pressure\",\"anxious\",\"decline\",\"emotion\",\"nerve-wracking\",\"loner\",\"pressure\",\"avoid\",\n",
    "\"stressed out\",\"stressed\",\"dislike\",\"shy\",\"hesitate\",\"losing\",\"bad\",\"difficult\",\"not great at networking\",\"obligation\",\"unfair\",\n",
    "\"stretch myself\",\"hate networking\",\"willing\",\"hell\",\"stress\",\"lose\",\"nightmare\",\"quit\",\"avoid going\",\"mad\",\"paid\",\"sad\",\n",
    "\"reluctant\",\"get out of it\",\"fair\",\"not a social person\",\"reluctance\",\"quiet\",\"upset\",\"sacrifice\",\"change\",\"not comfortable\",\n",
    "\"money\",\"tired\",\"family\",\"appeal\",\"confirm\",\"harm\",\"prove\",\"short \",\"skip\",\"stick\",\"hate\",\"compensate\",\"deserve\",\"short time\",\n",
    "\"sick\",\"outside of my comfort zone\",\"deadline\",\"pretend\",\"discomfort\",\"socially awkward\",\"show\",\"angry\",\"win\",\"convince\",\n",
    "\"not interested\",\"apply\",\"get along\",\"negotiate\",\"unpleasant\",\"quickly\",\"awkward\",\"not attend\",\"concerned\",\"plead\",\"fire\",\n",
    "\"suck it up\",\"forc\",\"comfortable\",\"pick\",\"uncomfortable\",\"unhappy\",\"excuse\",\"compromise\",\"afraid\",\"do not interact well with strangers\",\n",
    "\"don't like being in social situations\",\"don't like networking\",\"don't like socializing\",\"very shy\"]\n",
    "\n",
    "GO_3_LIST = [\"absolutely go\",\"all in\",\"attend\",\"attend that meeting\",\"certainly go\",\"cheerfully go\",\"decide to go\",\n",
    "\"definitely attend\",\"definitely be in attendance_1\",\"definitely go\",\"definitely still go\",\"go for it\",\"go for sure\",\n",
    "\"go to the event\",\"go to the meeting\",\"go to the networking meeting\",\"just go\",\"make an appearance\",\"make sure I go\",\n",
    "\"make time to attend\",\"still attend\",\"still go\",\"still opt in\",\"time and go\",\"would attend\",\"would go\",\"would still go\"]\n",
    "\n",
    "NOGO_3_LIST = [\"avoid\",\"backing out\",\"bow out of the meeting\",\"choose not to go\",\"come\",\"consider not going\",\"decide to go\",\n",
    "\"decline\",\"ditch\",\"get out of it\",\"go home\",\"happy to go\",\"hate going\",\"hesitate to go\",\"in attendance\",\"likely go\",\n",
    "\"likely not go\",\"might consider going\",\"no interest\",\"not attend\",\"not come\",\"not consider going\",\"not feel like going\",\n",
    "\"not going\",\"not interested\",\"not show up\",\"not volunteer\",\"not want to go\",\"politely decline\",\"probably attend\",\"probably go\",\n",
    "\"probably not go\",\"probably still go\",\"probably would not\",\"probably wouldn't\",\"skip\",\"stay at home\",\"try to go\",\"unlikely to go\",\n",
    "\"will not go\",\"would be going\",\"would not go\",\"wouldn't be going\",\"wouldn't do it\",\"wouldn't go\",\"wouldn't want to go\"]\n",
    "\n",
    "GO_5_LIST = ['would go','probably go']\n",
    "\n",
    "NOGO_5_LIST = ['not go','not to go',\"n't go\"]\n",
    "\n",
    "NOT_LIST = [\" not \"]\n",
    "\n",
    "NO_LIST = [\" no \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for counting word occurance\n",
    "\n",
    "def write_keyword_count_column(df, target_column, source_column, keyword_list):\n",
    "    def compute_keyword_list_count(text):\n",
    "        return sum([text.count(kw) for kw in keyword_list])    \n",
    "    df[target_column] = df[source_column].apply(compute_keyword_list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify key word list features\n",
    "\n",
    "write_keyword_count_column(df_total, 'O_high_5', 'open_ended_5', O_high_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'C_high_2', 'open_ended_2', C_high_2_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'A_high_1', 'open_ended_1', A_high_1_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'E_high_3', 'open_ended_3', E_high_3_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'A_low_2', 'open_ended_2', A_low_2_LIST)\n",
    "write_keyword_count_column(df_total, 'A_low_3', 'open_ended_3', A_low_3_LIST)\n",
    "write_keyword_count_column(df_total, 'A_low_4', 'open_ended_4', A_low_4_LIST)\n",
    "write_keyword_count_column(df_total, 'A_low_5', 'open_ended_5', A_low_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'N_low_1', 'open_ended_1', N_low_1_LIST)\n",
    "write_keyword_count_column(df_total, 'N_low_2', 'open_ended_2', N_low_2_LIST)\n",
    "write_keyword_count_column(df_total, 'N_low_3', 'open_ended_3', N_low_3_LIST)\n",
    "write_keyword_count_column(df_total, 'N_low_5', 'open_ended_5', N_low_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'C_high_1', 'open_ended_1', C_high_1_LIST)\n",
    "write_keyword_count_column(df_total, 'C_high_3', 'open_ended_3', C_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'C_high_4', 'open_ended_4', C_high_4_LIST)\n",
    "write_keyword_count_column(df_total, 'C_high_5', 'open_ended_5', C_high_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'A_high_2', 'open_ended_1', A_high_2_LIST)\n",
    "write_keyword_count_column(df_total, 'A_high_3', 'open_ended_3', A_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'A_high_4', 'open_ended_4', A_high_4_LIST)\n",
    "write_keyword_count_column(df_total, 'A_high_5', 'open_ended_5', A_high_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'N_high_1', 'open_ended_1', N_high_1_LIST)\n",
    "write_keyword_count_column(df_total, 'N_high_2', 'open_ended_2', N_high_2_LIST)\n",
    "write_keyword_count_column(df_total, 'N_high_3', 'open_ended_3', N_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'N_high_5', 'open_ended_5', N_high_5_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'O_high_1', 'open_ended_1', O_high_1_LIST)\n",
    "write_keyword_count_column(df_total, 'O_high_2', 'open_ended_2', O_high_2_LIST)\n",
    "write_keyword_count_column(df_total, 'O_high_3', 'open_ended_3', O_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'O_high_4', 'open_ended_4', O_high_4_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'E_high_3', 'open_ended_3', O_high_2_LIST)\n",
    "write_keyword_count_column(df_total, 'E_high_4', 'open_ended_4', O_high_3_LIST)\n",
    "write_keyword_count_column(df_total, 'E_high_5', 'open_ended_5', O_high_4_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'A_low_1', 'open_ended_1', A_low_1_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'E_low_3', 'open_ended_3', E_low_3_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'GO_3', 'open_ended_3', GO_3_LIST)\n",
    "write_keyword_count_column(df_total, 'NOGO_3', 'open_ended_3', NOGO_3_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'GO_5', 'open_ended_5', GO_3_LIST)\n",
    "write_keyword_count_column(df_total, 'NOGO_5', 'open_ended_5', NOGO_3_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'NOT_1', 'open_ended_1', NOT_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_2', 'open_ended_2', NOT_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_3', 'open_ended_3', NOT_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_4', 'open_ended_4', NOT_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_5', 'open_ended_5', NOT_LIST)\n",
    "\n",
    "write_keyword_count_column(df_total, 'NO_5', 'open_ended_5', NO_LIST)\n",
    "write_keyword_count_column(df_total, 'NOT_5', 'open_ended_5', NOT_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating aggregate features--combinations were derived in part from feedback from the public leaderboard\n",
    "\n",
    "df_total['A_low_comb'] = df_total['A_low_2']+df_total['A_low_3']+df_total['A_low_4']+df_total['A_low_5']\n",
    "df_total['N_low_comb'] = df_total['N_low_1']+df_total['N_low_2']+df_total['N_low_3']+df_total['N_low_5']\n",
    "df_total['C_high_comb'] = df_total['C_high_1']+df_total['C_high_3']+df_total['C_high_4']+df_total['C_high_5']\n",
    "df_total['A_high_comb'] = df_total['A_high_2']+df_total['A_high_3']+df_total['A_high_4']+df_total['A_high_5']\n",
    "df_total['N_high_comb'] = df_total['N_high_1']+df_total['N_high_2']+df_total['N_high_3']+df_total['N_high_5']\n",
    "df_total['O_high_comb'] = df_total['O_high_1']+df_total['O_high_2']+df_total['O_high_3']+df_total['O_high_4']\n",
    "df_total['E_high_3to5'] = df_total['E_high_3']+df_total['E_high_4']+df_total['E_high_5']\n",
    "\n",
    "df_total['A_not_comb'] = df_total['NOT_1']+df_total['NOT_2']+df_total['NOT_3']+df_total['NOT_4']+df_total['NOT_5']\n",
    "\n",
    "df_total['O_go_comb'] = df_total['GO_5']-df_total['NOGO_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building word lists 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['char_count_3'] = df_total['open_ended_3'].str.len() \n",
    "df_total['char_count_4'] = df_total['open_ended_4'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "df_total['avg_word_1'] = df_total['open_ended_1'].apply(lambda x: avg_word(x))\n",
    "df_total['avg_word_2'] = df_total['open_ended_2'].apply(lambda x: avg_word(x))\n",
    "df_total['avg_word_3'] = df_total['open_ended_3'].apply(lambda x: avg_word(x))\n",
    "df_total['avg_word_4'] = df_total['open_ended_4'].apply(lambda x: avg_word(x))\n",
    "df_total['avg_word_5'] = df_total['open_ended_5'].apply(lambda x: avg_word(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_list = [\" not \"]\n",
    "\n",
    "no_list = [\" no \"] #apply to 5 only\n",
    "\n",
    "e_high_3_list=['benefit','best','better','career','client','competition','confident','connect',\n",
    "'contact','contribute','convince','drink','enjoy','friendship','good','great','grow','importan',\n",
    "'impres','introduce','leader','learn','meet','miss out','missed out','network','open','oppurtunity',\n",
    "'party','positive','regardless','reward','sales','show','sociable','success','worthwhile']\n",
    "\n",
    "e_high_4_list=['benefit','best','better','career','client','competition','confident','connect',\n",
    "'contact','contribute','convince','drink','enjoy','friendship','good','great','grow','importan',\n",
    "'impres','introduce','leader','learn','meet','miss out','missed out','network','open','oppurtunity',\n",
    "'party','positive','regardless','reward','sales','show','sociable','success','worthwhile']\n",
    "\n",
    "e_high_5_list =['benefit','best','better','career','client','competition','confident','connect',\n",
    "'contact','contribute','convince','drink','enjoy','friendship','good','great','grow','importan',\n",
    "'impres','introduce','leader','learn','meet','miss out','missed out','network','open','oppurtunity',\n",
    "'party','positive','regardless','reward','sales','show','sociable','success','worthwhile']\n",
    "\n",
    "n_low_comb_emp_1_list=['as soon','report','show','problems','best','quickly','bonus','tense',\n",
    "'social','correct','win','concede','leader','misunderstand','unlikely','incorrect','fire',\n",
    "'easy going','paid','hesitate','human resources','time ','emotion','worried','racist','slash',\n",
    "'fun','valid','stubborn','flexible','review','beg','respect','benefit','open','threaten','short ',\n",
    "'change','first','trouble','agree','compromise','defend','defer','mad','harm']\n",
    "\n",
    "n_low_comb_emp_2_list=['worry','hate networking','client','responsible','longest','unhappy',\n",
    "'willing','accus','proof','difficult','family','anger','team','correct','consequence','comfortable',\n",
    "'stick','trouble','job','pressure','benefit','mad','report','deserve','accept','positive','review',\n",
    "'open','as soon','risk','time ','let','feel pressure','check in','depend','dislike','judg','social anxiety',\n",
    "'resent','lie','explain','upset','hard ','leader']\n",
    "\n",
    "n_low_comb_emp_3_list=['frustrated','learn','no problem','regardless','network','good','introduce',\n",
    "'anyone','definitely go','confident','meet','competition','contact','lie','client','I like parties',\n",
    "'not need anyone','great','social','party','worry','friendship','review','contribute','stretch myself',\n",
    "'surely attend','fool','plan ','help','leader','missed out','for sure go','fair','let','reluctance',\n",
    "'absolutely go','excited','happy to go','priority','excuse','hard ','report','job']\n",
    "\n",
    "n_low_comb_emp_5_list=['anger','learn','anyone','short time','help','leader','client','great time',\n",
    "'enjoy','importan','excited','hesitate','correct','lie','team','losing','career','responsible',\n",
    "'insist','immediately','bad','happy to go','pretend','willing','emotion','short ','stress','confus',\n",
    "'trouble','time ','worry','success','regardless','report','hurt','show','money','contact','stick',\n",
    "'mad','unlikely']\n",
    "\n",
    "n_high_comb_emp_1_list=['willing','regardless','losing','great','shy','career','obligated','organize',\n",
    "'stick','forc','appeal','anger','unfair','positive','early','reward','my right','I had to','refuse',\n",
    "'money','negotiate','personal issue','wrong','anyone','family','enjoy','pissed','hard ','team',\n",
    "'deny','insist','busy','sacrifice','skip','proof','fair','client','better','contact','meet','question',\n",
    "'fool','get even','profanity','cold','unhappy','angry','call in','awkward','excuse','upset','get along',\n",
    "'demand','lose','avoid','deadline','stressed','unpleasant','terrible','difficult','frustrated','confront',\n",
    "'hell','plead','alone','improve','stare','concerned','hardship','nice','pressure','sad','reflect','probably',\n",
    "'friendship','reluctant','sick','obligation','quit','hate','offer','hard stance']\n",
    "\n",
    "n_high_comb_emp_2_list=['calm','panic','stress','enjoy','bonus','show','learn','question','decline',\n",
    "'sick','importan','colleagues','worried','worry','connect','meet','rage','paid','pretend','anxiety',\n",
    "'avoid going','lose','early','bad','angry','better','deadline','losing','hurt','priority','no problem',\n",
    "'wrong','demand','beg','I had to','busy','compromise','negotiate','probably']\n",
    "\n",
    "n_high_comb_emp_3_list=['wrong','compromise','respect','risk','show','afraid','bonus','worried','tense',\n",
    "'worthwhile','dislike','valid','confirm','socially awkward','introvert','losing','deserve','quickly',\n",
    "'beg','plead','mad','change','better','angry','apply','not comfortable','lose','get out of it',\n",
    "'agree','paid','outside of my comfort zone','not a social person','miss out','time ','family','reconsider',\n",
    "'anxious','short time','prove','negative','money','fire','tired','negotiate','I had to','harm','appeal',\n",
    "'sacrifice','hell','stress','awkward','forc','hesitate','pressure','trouble','willing','deadline','short ',\n",
    "'suck it up','get along','loner','stressed','resent','skip','social anxiety','bad','not great at networking',\n",
    "'nightmare','shy','avoid','impres','concerned','difficult','probably','compensate','emotion','unpleasant',\n",
    "'obligation','nervous','feel pressure','extremely uncomfortable','nerve-wracking','hate networking','immediately',\n",
    "'hate','would not go','social anxious','panic','unlikely','discomfort','not go','anxiety']\n",
    "\n",
    "n_high_comb_emp_5_list=['negative','allow','best','hate networking','let','positive','apply','anger',\n",
    "'beg','bonus','comfortable','dislike','oppurtunity','obligation','improve','concerned','pick','open',\n",
    "'right away','job','rage','probably','refuse','upset','afraid','risk','alone','social anxiety','consequence',\n",
    "'agree','prove','fair','colleagues','awkward','paid','grow','avoid going','early','nervous','forc',\n",
    "'depend','resent','frustrated','difficult']\n",
    "\n",
    "e_low_3_emp_list=['social anxiety','extremely uncomfortable','nervous','social anxious','panic','unlikely',\n",
    "'impres','anxiety','probably','introvert','immediately','feel pressure','anxious','decline','emotion',\n",
    "'nerve-wracking','loner','pressure','avoid','stressed out','stressed','dislike','shy','hesitate','losing',\n",
    "'bad','difficult','not great at networking','obligation','unfair','stretch myself','hate networking',\n",
    "'willing','hell','stress','lose','nightmare','quit','avoid going','mad','paid','sad','reluctant','get out of it',\n",
    "'fair','not a social person','reluctance','quiet','upset','sacrifice','change','not comfortable','money',\n",
    "'tired','family','appeal','confirm','harm','prove','short ','skip','stick','hate','compensate','deserve',\n",
    "'short time','sick','outside of my comfort zone','deadline','pretend','discomfort','socially awkward',\n",
    "'show','angry','win','convince','not interested','apply','get along','negotiate','unpleasant','quickly',\n",
    "'awkward','not attend','concerned','plead','fire','suck it up','forc','comfortale','pick','uncomfortable',\n",
    "'unhappy','excuse','compromise','afraid','do not interact well with strangers',\"don't like being in social situation\",\n",
    "\"don't like networking\",\"don't like socializing\",'very shy']    \n",
    "\n",
    "e_high_3_emp_list=['career','good','frustrated','nice','best','deny','reflect','confident','grow','consequence',\n",
    "'missed out','connect','rage','importan','worry','I am sociable','party','right away','priority','sociable',\n",
    "'accept','focus','plan ','report','excited','reward','contribute','allow','success','contact','review',\n",
    "'absolutely go','for sure go','meet','great','colleagues','social','not need anyone','regardless','fool',\n",
    "'surely attend','leader','network','I like parties','no problem','learn','friendship','definitely go','introduce',\n",
    "'let','competition','client','make new friends']\n",
    "\n",
    "c_high_2_emp_list=['family','report','stress','question','convince','job','deserve','longest','comfortable','win','great time',\n",
    "'negative','fair','check in','short time','accus','short ','respect','willing','lie','correct','as soon',\n",
    "'positive','impres','review','problems','immediately','hate networking','anger','proof','upset','prove',\n",
    "'open','explain','improve','time ','confident','right away','let']    \n",
    "\n",
    "c_low_comb_emp_1_list=['hard stance','hardship','my right','call in','stare','contact','shy','quit','entitle','sad','reluctant','refuse',\n",
    "'demand','fool','wrong','get even','profanity','compensate','pressure','convince','responsible','fair','prove',\n",
    "'client','skip','get along','negotiate','sick','family','hell','difficult','sacrifice','obligation','awkward',\n",
    "'offer','friendship','hard ','career','threaten','party','allow','avoid','stick','hate','improve','terrible',\n",
    "'deadline','plan ','great','personal','plead','trouble','marked','respect','probably','early','confront',\n",
    "'lose','suck it up','better','excuse','suffer','busy','money','stress','beg','unfair','time ','personal issue',\n",
    "'deny','stressed']\n",
    "    \n",
    "c_low_comb_emp_3_list=['obligation','unlikely','panic','would not go','discomfort','deserve','awkward','immediately','unhappy',\n",
    "'social anxiety','skip','forc','social anxious','harm','probably','unpleasant','worthwhile','oppurtunity',\n",
    "'fire','negative','get along','shy','team','short ','open','pick','feel pressure','show','quit','angry',\n",
    "'avoid','hell','confirm','decline','money','socially awkward','negotiate','extremely uncomfortable','time ',\n",
    "'short time','not go','better','avoid going','not a social person','loner','valid','anger','hesitate',\n",
    "'hate networking','great time','positive','sociable','prove','emotion','compromise','allow','rage','concerned',\n",
    "'anxiety','outside of my comfort zone','wrong','deadline','stress','resent','personal issue','reconsider',\n",
    "'cold','not interested','unfair','early','drink','stressed']\n",
    "\n",
    "c_low_comb_emp_4_list=['probably','career','appeal','anxiety','depend','wrong','sad','cold','fool','marked','reflect','not go',\n",
    "'harm','bad','hate networking','job','mad','money','would change','reward','quit','focus','organize',\n",
    "'stressed','hesitate','leader','valid','difficult','consequence','emotion','personal issue','bonus',\n",
    "'accept','review','nice','avoid going','plan ','great time','lose','fire','frustrated','pressure','confront']\n",
    "    \n",
    "c_low_comb_emp_5_list=['forc','network','resent','considerate','difficult','I had to','frustrated','paid','obligation','nervous',\n",
    "'refuse','explain','rage','anger','grow','meet','respect','oppurtunity','bonus','nice','insist','job',\n",
    "'depend','avoid going','upset','awkward','positive','apply','short ','connect','probably','plan ','win',\n",
    "'open','concerned','question','negative','change','friendship','dislike','focus','alone']\n",
    "    \n",
    "c_high_comb_emp_1_list=['question','stubborn','would change','reconsider','as soon','human resources','disagree','defer','risk',\n",
    "'unpleasant','immediately','worry','argue','petty','explain','mad','proof','hurt','correct','obligated',\n",
    "'not go','harm','unhappy','leader','misunderstand','win','fire','unlikely','first','pick','angry','priority',\n",
    "'bonus','quickly','short time','hesitate','tense','social','switch','success','problems','not interested',\n",
    "'easy going','no problem','report','reflect','upset','anger','team','valid','paid','review','agree',\n",
    "'short ','willing','fun','concede','show','seniority','flexible','change']\n",
    "    \n",
    "c_high_comb_emp_3_list=['no problem','introvert','frustrated','win','sad','missed out','alone','dislike','appeal','depend','insist',\n",
    "'sick','success','not comfortable','stretch myself','report','benefit','accept','hard ','contribute','responsible',\n",
    "'compensate','fool','social','absolutely go','regardless','anyone','focus','pretend','worried','nightmare',\n",
    "'not need anyone','surely attend','for sure go','colleagues','competition','let','help','best','great',\n",
    "'deny','importan','learn','network','client','uncomfortable','priority','lie','improve','good','not attend',\n",
    "'fun','definitely go','mad','comfortable','reluctant','excited','excuse','meet']\n",
    "    \n",
    "c_high_comb_emp_4_list=['willing','change','respect','connect','fun','paid','hard ','immediately','terrible','grow','incorrect',\n",
    "'refuse','open','resent','quickly','contact','calm','party','short ','contribute','my right','stubborn',\n",
    "'rebut','problems','worried','as soon','compromise','hurt','good','proof','not true','early','human resources',\n",
    "'obligation','colleagues','meet','demand','success','negative','allow','concerned','disagree','let','agree']    \n",
    "    \n",
    "\n",
    "c_high_comb_emp_5_list=['success','appeal','worry','fun','busy','hesitate','problems','allow','hurt','improve','excited','good',\n",
    "'bad','leader','stress','importan','excuse','introduce','lose','enjoy','prove','personal issue','fair',\n",
    "'quickly','correct','stick','accus','unlikely','comfortable','sad','willing','contact','confus','career',\n",
    "'show','losing','immediately','compensate','anyone','lie','client','help','learn']\n",
    "    \n",
    "a_low_emp_1_list=['stare','responsible','fool','get even','profanity','call in','sick','refuse','emotion','hard stance','racist',\n",
    "'slash','hardship','demand','compensate','first','stick','quit','personal issue','excuse','trouble','deny',\n",
    "'hell','depend','money','cold','hard ','marked','pissed','client','deserve','unfair','fair',\n",
    "'resent','reconsider','offer','my right','hate','forc','worry','reward','reluctant','concerned','organize',\n",
    "'sad','losing','rage','bad','insist','busy','difficult','appeal','stressed out','stressed','wrong',\n",
    "'early','longest','proof','better','petty','improve','contact','avoid','accept','entitle','meet',\n",
    "'if I had to','seniority','suffer','comfortable','regardless','personal','not back down','stand my ground']  \n",
    "    \n",
    "a_high_emp_1_list=['agree','benefit','best','bonus','change','compromise','considerate','correct','defer','easy going','family',\n",
    "'flexible','fun','good','help','hurt','incorrect','leader','let','misunderstand','no problem','not interested',\n",
    "'obligation','paid','pick','priority','problems','quickly','respect','review','show','willing','win',\n",
    "'more than willing',\"don't want conflict\",'easy going','hate conflict','team player','avoid confrontation',\n",
    "'keep people happy']  \n",
    "    \n",
    "a_low_comb_emp_2_list=['wrong','question','busy','probably','resent','not go','importan','fun','enjoy','bad','first','problems',\n",
    "'refuse','better','short time','good','anxiety','avoid going','respect','compromise','losing','angry',\n",
    "'regardless','social anxiety','rage','decline','pretend','focus','connect','no problem','priority','excuse',\n",
    "'procrastinate','fool','sick','personal issue','anticipat','deadline','anyone','lose','difficult','meet',\n",
    "'judg','worry','plan ','trouble','show','nervous','reflect','help','pressure','compensate','bonus','get along',\n",
    "'flexible','colleagues','accus','fire','consequence','demand']\n",
    "    \n",
    "a_low_comb_emp_3_list=['I like parties','fire','unpleasant','would not go','sales','quit','discomfort','money','hate networking',\n",
    "'worthwhile','obligation','panic','emotion','unlikely','hell','skip','social anxious','pick','cold',\n",
    "'decline','not go','paid','get out of it','hate','reward','rage','short ','negotiate','beg','difficult',\n",
    "'trouble','resent','time ','immediately','stress','stressed','stressed out','reconsider','short time',\n",
    "'grow','extremely uncomfortable','willing','get along','apply','I had to','risk','anxiety','great',\n",
    "'forc','allow','socially awkward','dislike','I am sociable','great time','missed out','compensate','oppurtunity',\n",
    "'anger','benefit','plan ','confirm','avoid','social anxiety','fair','pressure','mad','deserve',\n",
    "'not a social person']\n",
    "    \n",
    "a_low_comb_emp_4_list=['rage','cold','fool','marked','depend','demand','quit','report','probably','career','accept','not go',\n",
    "'compensate','pressure','quiet','angry','afraid','confront','emotion','job','benefit','mad','threaten',\n",
    "'money','unpleasant','anxiety','pissed','anyone','obligation','confident','short ','regardless','refuse','appeal',\n",
    "'hesitate','examples','immediately','bad','suck it up','resent','respect','wrong','harm']\n",
    "    \n",
    "a_low_comb_emp_5_list=['paid','refuse','avoid going','alone','emotion','pretend','resent','bonus','win','rage','difficult',\n",
    "'probably','afraid','anger','forc','hate networking','change','agree','depend','pick','focus','obligation',\n",
    "'frustrated','considerate','right away','time ','money','negative','colleagues','awkward','improve','success',\n",
    "'explain','bad','best','respect','let','better','nice','nervous']\n",
    "    \n",
    "a_high_comb_emp_2_list=['agree','negative','benefit','overwhelmed','quiet','I had to','lie','team','check in','early','stick',\n",
    "'feel pressure','allow','family','sacrifice','stressed','learn','frustrated','right away','convince',\n",
    "'best','let','fair','client','longest','responsible','mad','stressed out','report','time ','upset',\n",
    "'confident','dislike','unhappy','anger','explain','positive','stress','proof']\n",
    "    \n",
    "a_high_comb_emp_3_list=['change','reluctant','angry','quickly','right away','excuse','stick','would change','early','compromise',\n",
    "'not comfortable','learn','positive','avoid going','anxious','colleagues','fool','reluctance','absolutely go',\n",
    "'fun','not attend','tired','losing','worry','busy','no problem','contribute','explain','hurt','network',\n",
    "'uncomfortable','consequence','social','not need anyone','surely attend','regardless','help','better',\n",
    "'excited','importan','priority','responsible','outside of my comfort zone','party','stretch myself','for sure go',\n",
    "'hard ','report','focus','client','alone','lie','introduce','friendship','comfortable','contact',\n",
    "'best','good','definitely go','anyone','meet']  \n",
    "    \n",
    "a_high_comb_emp_4_list=['convince','defend','lose','accus','worthwhile','agitated','personal','consequence','concerned','impres',\n",
    "'anger','success','correct','win','confus','argue','proof','incorrect','focus','terrible','best',\n",
    "'negative','not justified','as soon','plead','confirm','lie','unfair','early','judg','stressed out','hard ',\n",
    "'organize','risk','improve','worried','quickly','my right','open','frustrated','contact','meet',\n",
    "'compromise','pretend','rebut','stress','reconsider','hurt','would not go','importan','positive','problems',\n",
    "'agree','let','negotiate','allow','explain','learn','prove','better','anxious','colleagues','not true',\n",
    "'upset','grow']\n",
    "       \n",
    "a_high_comb_emp_5_list=['accept','short time','question','happy to go','excited','hard ','impres','good','grow','losing',\n",
    "'reward','show','contribute','convince','accus','willing','concerned','dislike','contact','hesitate',\n",
    "'network','comfortable','apply','leader','immediately','stress','correct','importan','great time','hurt',\n",
    "'offer','confus','help','anyone','lie','client','enjoy','learn']\n",
    "\n",
    "go_v2_3_list=['go for it','make an appearance','certainly go','would attend','just go','attend','still attend','still go',\n",
    "'would still go','definitely go','would go','all in','definitely be in attendance','absolutely go',\n",
    "'attend that meeting','cheerfully go','decide to go','definitely attend','definitely still go','go for sure',\n",
    "'go to the event','go to the meeting','go to the networking meeting','make sure I go','make time to attend',\n",
    "'still opt in','time and go']\n",
    "    \n",
    "not_go_v2_3_list=['would not go',\"wouldn't go\",'probably not go',\"wouldn't want to go\",'unlikely to go','decline',\n",
    "'not show up','hesitate to go','go home','ditch','avoid','get out of it','probably still go',\n",
    "'probably go','skip','decide to go','try to go','not going','in attendance','not interested',\n",
    "'come','not attend','would be going','not come','likely go','happy to go','probably attend',\n",
    "'bow out of the meeting','choose not to go','consider not going','hate going','likely not go',\n",
    "'not consider going','not feel like going','not want to go','politely decline','probably would not',\n",
    "\"probably wouldn't\",'stay at home','will not go']\n",
    "    \n",
    "not_go_5_list=['not go','not to go',\"n't go\"]\n",
    "\n",
    "go_5_list=['would go','probably go']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word List Section\n",
    "\n",
    "This section includes the code that was used in the word list prediction. The optimal weights were derived from feedback on the public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/359193904.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['n_high_comb_emp']=df_total['n_high_comb_emp_1']+df_total['n_high_comb_emp_2']+df_total['n_high_comb_emp_3']+df_total['n_high_comb_emp_5']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/359193904.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['e_high_3to5'] = df_total['e_high_3']+df_total['e_high_4']+df_total['e_high_5']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/359193904.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['c_low_comb_emp']=df_total['c_low_comb_emp_1']+df_total['c_low_comb_emp_3']+df_total['c_low_comb_emp_4']+df_total['c_low_comb_emp_5']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/359193904.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['c_high_comb_emp']=df_total['c_high_comb_emp_1']+df_total['c_high_comb_emp_3']+df_total['c_high_comb_emp_4']+df_total['c_high_comb_emp_5']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/359193904.py:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['a_low_comb_emp']=df_total['a_low_comb_emp_2']+df_total['a_low_comb_emp_2']+df_total['a_low_comb_emp_4']+df_total['a_low_comb_emp_5']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2318671464.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[target_column] = df[source_column].apply(compute_keyword_list_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/359193904.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['a_high_comb_emp']=df_total['a_high_comb_emp_2']+df_total['a_high_comb_emp_2']+df_total['a_high_comb_emp_4']+df_total['a_high_comb_emp_5']\n"
     ]
    }
   ],
   "source": [
    "# Compute new word count variables from word lists\n",
    "\n",
    "write_keyword_count_column(df_total, 'not_go_5', 'open_ended_5', not_go_5_list)\n",
    "write_keyword_count_column(df_total, 'go_5', 'open_ended_5', go_5_list)\n",
    "\n",
    "df_total['go_comb_5']=df_total['go_5']-df_total['not_go_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'not_1', 'open_ended_1', not_list)\n",
    "write_keyword_count_column(df_total, 'not_2', 'open_ended_2', not_list)\n",
    "write_keyword_count_column(df_total, 'not_3', 'open_ended_3', not_list)\n",
    "write_keyword_count_column(df_total, 'not_4', 'open_ended_4', not_list)\n",
    "write_keyword_count_column(df_total, 'not_5', 'open_ended_5', not_list)\n",
    "\n",
    "df_total['sum_not']=df_total['not_1']+df_total['not_2']+df_total['not_3']+df_total['not_4']+df_total['not_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'no_5', 'open_ended_5', no_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'n_low_comb_emp_1', 'open_ended_1', n_low_comb_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'n_low_comb_emp_2', 'open_ended_2', n_low_comb_emp_2_list)\n",
    "write_keyword_count_column(df_total, 'n_low_comb_emp_3', 'open_ended_3', n_low_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'n_low_comb_emp_5', 'open_ended_5', n_low_comb_emp_5_list)\n",
    "\n",
    "df_total['n_low_comb_emp']=df_total['n_low_comb_emp_1']+df_total['n_low_comb_emp_2']+df_total['n_low_comb_emp_3']+df_total['n_low_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'n_high_comb_emp_1', 'open_ended_1', n_high_comb_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'n_high_comb_emp_2', 'open_ended_2', n_high_comb_emp_2_list)\n",
    "write_keyword_count_column(df_total, 'n_high_comb_emp_3', 'open_ended_3', n_high_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'n_high_comb_emp_5', 'open_ended_5', n_high_comb_emp_5_list)\n",
    "\n",
    "df_total['n_high_comb_emp']=df_total['n_high_comb_emp_1']+df_total['n_high_comb_emp_2']+df_total['n_high_comb_emp_3']+df_total['n_high_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'e_high_3', 'open_ended_3', e_high_3_list)\n",
    "write_keyword_count_column(df_total, 'e_high_4', 'open_ended_4', e_high_4_list)\n",
    "write_keyword_count_column(df_total, 'e_high_5', 'open_ended_5', e_high_5_list)\n",
    "\n",
    "df_total['e_high_3to5'] = df_total['e_high_3']+df_total['e_high_4']+df_total['e_high_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'go_5', 'open_ended_5', go_5_list)\n",
    "write_keyword_count_column(df_total, 'not_go_5', 'open_ended_5', not_go_5_list)\n",
    "\n",
    "df_total['go_comb_5']=df_total['go_5']-df_total['not_go_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'go_v2', 'open_ended_3', go_v2_3_list)\n",
    "write_keyword_count_column(df_total, 'not_go_v2', 'open_ended_3', not_go_v2_3_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'c_high_2_emp', 'open_ended_2', c_high_2_emp_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'c_low_comb_emp_1', 'open_ended_1', c_low_comb_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'c_low_comb_emp_3', 'open_ended_2', c_low_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'c_low_comb_emp_4', 'open_ended_3', c_low_comb_emp_4_list)\n",
    "write_keyword_count_column(df_total, 'c_low_comb_emp_5', 'open_ended_5', c_low_comb_emp_5_list)\n",
    "\n",
    "df_total['c_low_comb_emp']=df_total['c_low_comb_emp_1']+df_total['c_low_comb_emp_3']+df_total['c_low_comb_emp_4']+df_total['c_low_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'c_high_comb_emp_1', 'open_ended_1', c_high_comb_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'c_high_comb_emp_3', 'open_ended_2', c_high_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'c_high_comb_emp_4', 'open_ended_3', c_high_comb_emp_4_list)\n",
    "write_keyword_count_column(df_total, 'c_high_comb_emp_5', 'open_ended_5', c_high_comb_emp_5_list)\n",
    "\n",
    "df_total['c_high_comb_emp']=df_total['c_high_comb_emp_1']+df_total['c_high_comb_emp_3']+df_total['c_high_comb_emp_4']+df_total['c_high_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'e_low_3_emp', 'open_ended_2', e_low_3_emp_list)\n",
    "write_keyword_count_column(df_total, 'e_high_3_emp', 'open_ended_2', e_high_3_emp_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'a_low_1_emp', 'open_ended_1', a_low_emp_1_list)\n",
    "write_keyword_count_column(df_total, 'a_high_1_emp', 'open_ended_1', a_high_emp_1_list)\n",
    "\n",
    "write_keyword_count_column(df_total, 'a_low_comb_emp_2', 'open_ended_1', a_low_comb_emp_2_list)\n",
    "write_keyword_count_column(df_total, 'a_low_comb_emp_3', 'open_ended_2', a_low_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'a_low_comb_emp_4', 'open_ended_3', a_low_comb_emp_4_list)\n",
    "write_keyword_count_column(df_total, 'a_low_comb_emp_5', 'open_ended_5', a_low_comb_emp_5_list)\n",
    "\n",
    "df_total['a_low_comb_emp']=df_total['a_low_comb_emp_2']+df_total['a_low_comb_emp_2']+df_total['a_low_comb_emp_4']+df_total['a_low_comb_emp_5']\n",
    "\n",
    "write_keyword_count_column(df_total, 'a_high_comb_emp_2', 'open_ended_1', a_high_comb_emp_2_list)\n",
    "write_keyword_count_column(df_total, 'a_high_comb_emp_3', 'open_ended_2', a_high_comb_emp_3_list)\n",
    "write_keyword_count_column(df_total, 'a_high_comb_emp_4', 'open_ended_3', a_high_comb_emp_4_list)\n",
    "write_keyword_count_column(df_total, 'a_high_comb_emp_5', 'open_ended_5', a_high_comb_emp_5_list)\n",
    "\n",
    "df_total['a_high_comb_emp']=df_total['a_high_comb_emp_2']+df_total['a_high_comb_emp_2']+df_total['a_high_comb_emp_4']+df_total['a_high_comb_emp_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of features to standardize\n",
    "\n",
    "zvarlist=['char_count_3',\n",
    " 'char_count_4',\n",
    " 'avg_word_1',\n",
    " 'avg_word_2',\n",
    " 'avg_word_3',\n",
    " 'avg_word_4',\n",
    " 'avg_word_5',\n",
    " 'not_go_5',\n",
    " 'go_5',\n",
    " 'go_comb_5',\n",
    " 'sum_not',\n",
    " 'no_5',\n",
    " 'not_5',\n",
    " 'n_low_comb_emp',\n",
    " 'n_high_comb_emp',\n",
    " 'e_high_3',\n",
    " 'e_high_4',\n",
    " 'e_high_5',\n",
    " 'e_high_3to5',\n",
    " 'go_v2',\n",
    " 'not_go_v2',\n",
    " 'c_high_2_emp',\n",
    " 'c_low_comb_emp',\n",
    " 'c_high_comb_emp',\n",
    " 'e_low_3_emp',\n",
    " 'e_high_3_emp',\n",
    " 'a_low_1_emp',\n",
    " 'a_high_1_emp',\n",
    " 'a_low_comb_emp',\n",
    " 'a_high_comb_emp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/988276864.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n"
     ]
    }
   ],
   "source": [
    "# Standardize list\n",
    "\n",
    "cols = zvarlist\n",
    "for col in cols:\n",
    "    col_zscore = 'Z'+ col\n",
    "    df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/4275134786.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['Za_low_1_emp ']=df_total['Za_low_1_emp']  *-1.5\n"
     ]
    }
   ],
   "source": [
    "# Weighting features\n",
    "\n",
    "df_total['Zno_5']=df_total['Zno_5'] * -1\n",
    "df_total['Znot_5']=df_total['Znot_5']  *-1\n",
    "df_total['Zn_low_comb_emp']=df_total['Zn_low_comb_emp']  *-1\n",
    "df_total['Zc_high_2_emp']=df_total['Zc_high_2_emp']  *1.25\n",
    "df_total['Zc_low_comb_emp']=df_total['Zc_low_comb_emp']  *-1\n",
    "df_total['Zc_high_comb_emp']=df_total['Zc_high_comb_emp'] *1.5\n",
    "df_total['Ze_low_3_emp']=df_total['Ze_low_3_emp']  *-1.25\n",
    "df_total['Ze_high_3_emp']=df_total['Ze_high_3_emp']  *1.25\n",
    "df_total['Znot_go_v2']=df_total['Znot_go_v2']  *-1\n",
    "df_total['Zsum_not']=df_total['Zsum_not']  *-1\n",
    "df_total['Za_low_1_emp ']=df_total['Za_low_1_emp']  *-1.5\n",
    "df_total['Za_low_comb_emp']=df_total['Za_low_comb_emp']  *-1\n",
    "df_total['Za_high_comb_emp']=df_total['Za_high_comb_emp']  *1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1612352425.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['o_pred']=df_total[['Zchar_count_3', 'Zchar_count_4', 'Zno_5', 'Znot_5', 'Ze_high_3to5',  'Zavg_word_2',\n"
     ]
    }
   ],
   "source": [
    "df_total['o_pred']=df_total[['Zchar_count_3', 'Zchar_count_4', 'Zno_5', 'Znot_5', 'Ze_high_3to5',  'Zavg_word_2', \n",
    "                         'Zavg_word_4', 'Zgo_comb_5']].mean(axis=1)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recode o_pred\n",
    "\n",
    "recode_list=df_total[['o_pred']]\n",
    "\n",
    "def recode_extreme(predictor_col):\n",
    "    if predictor_col >=0.75:\n",
    "        val=.75\n",
    "    else: \n",
    "        val=predictor_col\n",
    "    return val\n",
    "\n",
    "for predictor_col in recode_list:\n",
    "    df_total[predictor_col] = df_total[predictor_col].apply(recode_extreme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2606338093.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['n_pred'] = df_total[['Zn_low_comb_emp', 'Zn_high_comb_emp']].mean(axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_total['n_pred'] = df_total[['Zn_low_comb_emp', 'Zn_high_comb_emp']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3012799104.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['c_pred'] = df_total[['Zc_high_2_emp','Zc_low_comb_emp','Zc_high_comb_emp','Zavg_word_5']].mean(axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_total['c_pred'] = df_total[['Zc_high_2_emp','Zc_low_comb_emp','Zc_high_comb_emp','Zavg_word_5']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3136581782.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['e_pred'] = df_total[['Ze_low_3_emp', 'Ze_high_3_emp', 'Zgo_v2', 'Znot_go_v2']].mean(axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_total['e_pred'] = df_total[['Ze_low_3_emp', 'Ze_high_3_emp', 'Zgo_v2', 'Znot_go_v2']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2236334782.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['a_pred'] = df_total[['Zavg_word_4', 'Zsum_not', 'Zavg_word_5', 'Za_low_1_emp', 'Za_high_1_emp', 'Za_low_comb_emp','Za_high_comb_emp']].mean(axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_total['a_pred'] = df_total[['Zavg_word_4', 'Zsum_not', 'Zavg_word_5', 'Za_low_1_emp', 'Za_high_1_emp', 'Za_low_comb_emp','Za_high_comb_emp']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Machine Learning Section\n",
    "\n",
    "Much of the machine learning that we applied did not result in stronger predictions on the public leaderboard compare to the word lists. Therefore, much of these exploratory features have been removed. We retained what was ultimately submitted to the private leaderboard.\n",
    "\n",
    "Not all the features that are defined here are important to the prediction. Again, we were lazy and did not prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllables_count(text): \n",
    "    return textstatistics().syllable_count(text) \n",
    "\n",
    "def difficult_word_count(text):\n",
    "    return textstatistics().difficult_words(text)\n",
    "\n",
    "def sentence_count(text):\n",
    "    return textstatistics().sentence_count(text)\n",
    "\n",
    "def avg_syllables_per_word(text): \n",
    "    nsyllables=syllables_count(text)\n",
    "    nwords=word_count(text)\n",
    "    ASPW=float(nsyllables)/float(nwords)\n",
    "    return legacy_round(ASPW,2)\n",
    "\n",
    "def avg_sentence_length(text): \n",
    "    nwords = word_count(text) \n",
    "    nsentences = sentence_count(text) \n",
    "    average_sentence_length = float(nwords / nsentences) \n",
    "    return legacy_round(average_sentence_length,2)\n",
    "  \n",
    "def flesch_ease_score(text):\n",
    "    return textstatistics().flesch_reading_ease(text)\n",
    "    \n",
    "def flesch_grade_score(text):\n",
    "    return textstatistics().flesch_kincaid_grade(text)\n",
    "\n",
    "def linsear_write_score(text):\n",
    "    return textstatistics().linsear_write_formula(text)\n",
    "\n",
    "def dale_chall_score(text):\n",
    "    return textstatistics().dale_chall_readability_score(text)\n",
    "\n",
    "def gunning_fog_score(text):\n",
    "    return textstatistics().gunning_fog(text)\n",
    "\n",
    "def smog_score(text):\n",
    "    return textstatistics().smog_index(text)\n",
    "\n",
    "def automated_readability_score(text):\n",
    "    return textstatistics().automated_readability_index(text)\n",
    "\n",
    "def coleman_liau_score(text):\n",
    "    return textstatistics().coleman_liau_index(text)\n",
    "\n",
    "# This function is supposed to count grammatical errors. \n",
    "# def lang_checker(text):\n",
    "#     tool = language_check.LanguageTool('en-US')\n",
    "#     count=0\n",
    "#     matches = tool.check(text)\n",
    "#     for i in range(len(matches)-1):\n",
    "#         if matches[i].ruleId == 'WHITESPACE_RULE':\n",
    "#             pass\n",
    "#         else:\n",
    "#             count+=1\n",
    "#     return count\n",
    "\n",
    "def lang_checker(text):\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    misspelled = spell.unknown(words)\n",
    "\n",
    "    # Count misspelled words\n",
    "    spell_errors = len(misspelled)\n",
    "\n",
    "    return spell_errors\n",
    "\n",
    "def tokenize(text):\n",
    "    return TextBlob(text).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_num_chars\"] = df_total[predictor_col].apply(len)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_flesch_grade\"] = df_total[predictor_col].apply(flesch_grade_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_num_chars\"] = df_total[predictor_col].apply(len)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_flesch_grade\"] = df_total[predictor_col].apply(flesch_grade_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_num_chars\"] = df_total[predictor_col].apply(len)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_flesch_grade\"] = df_total[predictor_col].apply(flesch_grade_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_num_chars\"] = df_total[predictor_col].apply(len)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_flesch_grade\"] = df_total[predictor_col].apply(flesch_grade_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_num_chars\"] = df_total[predictor_col].apply(len)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_flesch_grade\"] = df_total[predictor_col].apply(flesch_grade_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_num_chars\"] = df_total[predictor_col].apply(len)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[predictor_col + \"_flesch_grade\"] = df_total[predictor_col].apply(flesch_grade_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_syllables_count\"] = df_total['open_ended_6'].apply(syllables_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_word_count\"] = df_total['open_ended_6'].apply(word_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_difficult_count\"] = df_total['open_ended_6'].apply(difficult_word_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_sentence_count\"] = df_total['open_ended_6'].apply(sentence_count)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_avg_syllables_per_word\"] = df_total['open_ended_6'].apply(avg_syllables_per_word)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_avg_sentence_length\"] = df_total['open_ended_6'].apply(avg_sentence_length)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_flesch_ease_score\"] = df_total['open_ended_6'].apply(flesch_ease_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_flesch_grade_score\"] = df_total['open_ended_6'].apply(flesch_grade_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_linsear_write_score\"] = df_total['open_ended_6'].apply(linsear_write_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_dale_chall_score\"] = df_total['open_ended_6'].apply(dale_chall_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_smog_score\"] = df_total['open_ended_6'].apply(smog_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"readability_coleman_liau_score\"] = df_total['open_ended_6'].apply(coleman_liau_score)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[\"number_grammar_errors\"] = df_total['open_ended_6'].apply(lang_checker)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['Avg_word_length_1']=df_total['open_ended_1_num_chars']/df_total['open_ended_1_num_words']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['Avg_word_length_2']=df_total['open_ended_2_num_chars']/df_total['open_ended_2_num_words']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['Avg_word_length_3']=df_total['open_ended_3_num_chars']/df_total['open_ended_3_num_words']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['Avg_word_length_4']=df_total['open_ended_4_num_chars']/df_total['open_ended_4_num_words']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['Avg_word_length_5']=df_total['open_ended_5_num_chars']/df_total['open_ended_5_num_words']\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/1188796333.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total['Avg_word_length_6']=df_total['open_ended_6_num_chars']/df_total['open_ended_6_num_words']\n"
     ]
    }
   ],
   "source": [
    "# Compute some spelling-based features\n",
    "for predictor_col in PREDICTOR_TEXT_COLUMN_NAMES_ALL:\n",
    "    df_total[predictor_col + \"_num_chars\"] = df_total[predictor_col].apply(len)\n",
    "    df_total[predictor_col + \"_num_words\"] = df_total[predictor_col].apply(word_count)\n",
    "    df_total[predictor_col + \"_num_misspelled\"] = df_total[predictor_col].apply(compute_num_spelling_errors)\n",
    "    df_total[predictor_col + \"_flesch_grade\"] = df_total[predictor_col].apply(flesch_grade_score) \n",
    "    df_total[predictor_col + \"_percent_misspelled\"] = df_total[[predictor_col + \"_num_misspelled\",\n",
    "                              predictor_col + \"_num_words\"\n",
    "    ]].apply(lambda x: divide(*x), axis=1)\n",
    "\n",
    "# Compute readability features\n",
    "df_total[\"readability_syllables_count\"] = df_total['open_ended_6'].apply(syllables_count) \n",
    "df_total[\"readability_word_count\"] = df_total['open_ended_6'].apply(word_count) \n",
    "df_total[\"readability_difficult_count\"] = df_total['open_ended_6'].apply(difficult_word_count) \n",
    "df_total[\"readability_sentence_count\"] = df_total['open_ended_6'].apply(sentence_count) \n",
    "df_total[\"readability_avg_syllables_per_word\"] = df_total['open_ended_6'].apply(avg_syllables_per_word)\n",
    "df_total[\"readability_avg_sentence_length\"] = df_total['open_ended_6'].apply(avg_sentence_length) \n",
    "df_total[\"readability_flesch_ease_score\"] = df_total['open_ended_6'].apply(flesch_ease_score) \n",
    "df_total[\"readability_flesch_grade_score\"] = df_total['open_ended_6'].apply(flesch_grade_score) \n",
    "df_total[\"readability_linsear_write_score\"] = df_total['open_ended_6'].apply(linsear_write_score) \n",
    "df_total[\"readability_dale_chall_score\"] = df_total['open_ended_6'].apply(dale_chall_score) \n",
    "df_total[\"readability_smog_score\"] = df_total['open_ended_6'].apply(smog_score) \n",
    "df_total[\"readability_coleman_liau_score\"] = df_total['open_ended_6'].apply(coleman_liau_score) \n",
    "\n",
    "# Compute variable for the number of grammar errors based on Nick's function. \n",
    "df_total[\"number_grammar_errors\"] = df_total['open_ended_6'].apply(lang_checker)\n",
    "  \n",
    "# Compute Average Word Length for each open ended comment.     \n",
    "df_total['Avg_word_length_1']=df_total['open_ended_1_num_chars']/df_total['open_ended_1_num_words']\n",
    "df_total['Avg_word_length_2']=df_total['open_ended_2_num_chars']/df_total['open_ended_2_num_words']\n",
    "df_total['Avg_word_length_3']=df_total['open_ended_3_num_chars']/df_total['open_ended_3_num_words']\n",
    "df_total['Avg_word_length_4']=df_total['open_ended_4_num_chars']/df_total['open_ended_4_num_words']\n",
    "df_total['Avg_word_length_5']=df_total['open_ended_5_num_chars']/df_total['open_ended_5_num_words']\n",
    "df_total['Avg_word_length_6']=df_total['open_ended_6_num_chars']/df_total['open_ended_6_num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A_Scale_score',\n",
       " 'C_Scale_score',\n",
       " 'E_Scale_score',\n",
       " 'N_Scale_score',\n",
       " 'O_Scale_score',\n",
       " 'Respondent_ID',\n",
       " 'open_ended_1_num_words',\n",
       " 'open_ended_1_num_misspelled',\n",
       " 'open_ended_1_percent_misspelled',\n",
       " 'open_ended_2_num_words',\n",
       " 'open_ended_2_num_misspelled',\n",
       " 'open_ended_2_percent_misspelled',\n",
       " 'open_ended_3_num_words',\n",
       " 'open_ended_3_num_misspelled',\n",
       " 'open_ended_3_percent_misspelled',\n",
       " 'open_ended_4_num_words',\n",
       " 'open_ended_4_num_misspelled',\n",
       " 'open_ended_4_percent_misspelled',\n",
       " 'open_ended_5_num_words',\n",
       " 'open_ended_5_num_misspelled',\n",
       " 'open_ended_5_percent_misspelled',\n",
       " 'open_ended_6_num_words',\n",
       " 'open_ended_6_num_misspelled',\n",
       " 'open_ended_6_percent_misspelled',\n",
       " 'O_high_5',\n",
       " 'C_high_2',\n",
       " 'A_high_1',\n",
       " 'E_high_3',\n",
       " 'A_low_2',\n",
       " 'A_low_3',\n",
       " 'A_low_4',\n",
       " 'A_low_5',\n",
       " 'N_low_1',\n",
       " 'N_low_2',\n",
       " 'N_low_3',\n",
       " 'N_low_5',\n",
       " 'C_high_1',\n",
       " 'C_high_3',\n",
       " 'C_high_4',\n",
       " 'C_high_5',\n",
       " 'A_high_2',\n",
       " 'A_high_3',\n",
       " 'A_high_4',\n",
       " 'A_high_5',\n",
       " 'N_high_1',\n",
       " 'N_high_2',\n",
       " 'N_high_3',\n",
       " 'N_high_5',\n",
       " 'O_high_1',\n",
       " 'O_high_2',\n",
       " 'O_high_3',\n",
       " 'O_high_4',\n",
       " 'E_high_4',\n",
       " 'E_high_5',\n",
       " 'A_low_1',\n",
       " 'E_low_3',\n",
       " 'GO_3',\n",
       " 'NOGO_3',\n",
       " 'GO_5',\n",
       " 'NOGO_5',\n",
       " 'NOT_1',\n",
       " 'NOT_2',\n",
       " 'NOT_3',\n",
       " 'NOT_4',\n",
       " 'NOT_5',\n",
       " 'NO_5',\n",
       " 'A_low_comb',\n",
       " 'N_low_comb',\n",
       " 'C_high_comb',\n",
       " 'A_high_comb',\n",
       " 'N_high_comb',\n",
       " 'O_high_comb',\n",
       " 'E_high_3to5',\n",
       " 'A_not_comb',\n",
       " 'O_go_comb',\n",
       " 'char_count_3',\n",
       " 'char_count_4',\n",
       " 'avg_word_1',\n",
       " 'avg_word_2',\n",
       " 'avg_word_3',\n",
       " 'avg_word_4',\n",
       " 'avg_word_5',\n",
       " 'not_go_5',\n",
       " 'go_5',\n",
       " 'go_comb_5',\n",
       " 'not_1',\n",
       " 'not_2',\n",
       " 'not_3',\n",
       " 'not_4',\n",
       " 'not_5',\n",
       " 'sum_not',\n",
       " 'no_5',\n",
       " 'n_low_comb_emp_1',\n",
       " 'n_low_comb_emp_2',\n",
       " 'n_low_comb_emp_3',\n",
       " 'n_low_comb_emp_5',\n",
       " 'n_low_comb_emp',\n",
       " 'n_high_comb_emp_1',\n",
       " 'n_high_comb_emp_2',\n",
       " 'n_high_comb_emp_3',\n",
       " 'n_high_comb_emp_5',\n",
       " 'n_high_comb_emp',\n",
       " 'e_high_3',\n",
       " 'e_high_4',\n",
       " 'e_high_5',\n",
       " 'e_high_3to5',\n",
       " 'go_v2',\n",
       " 'not_go_v2',\n",
       " 'c_high_2_emp',\n",
       " 'c_low_comb_emp_1',\n",
       " 'c_low_comb_emp_3',\n",
       " 'c_low_comb_emp_4',\n",
       " 'c_low_comb_emp_5',\n",
       " 'c_low_comb_emp',\n",
       " 'c_high_comb_emp_1',\n",
       " 'c_high_comb_emp_3',\n",
       " 'c_high_comb_emp_4',\n",
       " 'c_high_comb_emp_5',\n",
       " 'c_high_comb_emp',\n",
       " 'e_low_3_emp',\n",
       " 'e_high_3_emp',\n",
       " 'a_low_1_emp',\n",
       " 'a_high_1_emp',\n",
       " 'a_low_comb_emp_2',\n",
       " 'a_low_comb_emp_3',\n",
       " 'a_low_comb_emp_4',\n",
       " 'a_low_comb_emp_5',\n",
       " 'a_low_comb_emp',\n",
       " 'a_high_comb_emp_2',\n",
       " 'a_high_comb_emp_3',\n",
       " 'a_high_comb_emp_4',\n",
       " 'a_high_comb_emp_5',\n",
       " 'a_high_comb_emp',\n",
       " 'Zchar_count_3',\n",
       " 'Zchar_count_4',\n",
       " 'Zavg_word_1',\n",
       " 'Zavg_word_2',\n",
       " 'Zavg_word_3',\n",
       " 'Zavg_word_4',\n",
       " 'Zavg_word_5',\n",
       " 'Znot_go_5',\n",
       " 'Zgo_5',\n",
       " 'Zgo_comb_5',\n",
       " 'Zsum_not',\n",
       " 'Zno_5',\n",
       " 'Znot_5',\n",
       " 'Zn_low_comb_emp',\n",
       " 'Zn_high_comb_emp',\n",
       " 'Ze_high_3',\n",
       " 'Ze_high_4',\n",
       " 'Ze_high_5',\n",
       " 'Ze_high_3to5',\n",
       " 'Zgo_v2',\n",
       " 'Znot_go_v2',\n",
       " 'Zc_high_2_emp',\n",
       " 'Zc_low_comb_emp',\n",
       " 'Zc_high_comb_emp',\n",
       " 'Ze_low_3_emp',\n",
       " 'Ze_high_3_emp',\n",
       " 'Za_low_1_emp',\n",
       " 'Za_high_1_emp',\n",
       " 'Za_low_comb_emp',\n",
       " 'Za_high_comb_emp',\n",
       " 'Za_low_1_emp ',\n",
       " 'o_pred',\n",
       " 'n_pred',\n",
       " 'c_pred',\n",
       " 'e_pred',\n",
       " 'a_pred',\n",
       " 'open_ended_1_num_chars',\n",
       " 'open_ended_1_flesch_grade',\n",
       " 'open_ended_2_num_chars',\n",
       " 'open_ended_2_flesch_grade',\n",
       " 'open_ended_3_num_chars',\n",
       " 'open_ended_3_flesch_grade',\n",
       " 'open_ended_4_num_chars',\n",
       " 'open_ended_4_flesch_grade',\n",
       " 'open_ended_5_num_chars',\n",
       " 'open_ended_5_flesch_grade',\n",
       " 'open_ended_6_num_chars',\n",
       " 'open_ended_6_flesch_grade',\n",
       " 'readability_syllables_count',\n",
       " 'readability_word_count',\n",
       " 'readability_difficult_count',\n",
       " 'readability_sentence_count',\n",
       " 'readability_avg_syllables_per_word',\n",
       " 'readability_avg_sentence_length',\n",
       " 'readability_flesch_ease_score',\n",
       " 'readability_flesch_grade_score',\n",
       " 'readability_linsear_write_score',\n",
       " 'readability_dale_chall_score',\n",
       " 'readability_smog_score',\n",
       " 'readability_coleman_liau_score',\n",
       " 'number_grammar_errors',\n",
       " 'Avg_word_length_1',\n",
       " 'Avg_word_length_2',\n",
       " 'Avg_word_length_3',\n",
       " 'Avg_word_length_4',\n",
       " 'Avg_word_length_5',\n",
       " 'Avg_word_length_6']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of the numeric columns to paste into the Z-Score variable list\n",
    "FEATURES = df_total.select_dtypes(include=[np.number]).columns.tolist()\n",
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/2800878354.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)\n"
     ]
    }
   ],
   "source": [
    "# Create Z Scores for all new feastures\n",
    "\n",
    "cols = FEATURES\n",
    "for col in cols:\n",
    "    col_zscore = 'Z_'+ col\n",
    "    df_total[col_zscore] = (df_total[col] - df_total[col].mean())/df_total[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_Var_List=[\n",
    " 'Z_open_ended_1_num_words',\n",
    " 'Z_open_ended_1_num_misspelled',\n",
    " 'Z_open_ended_1_percent_misspelled',\n",
    " 'Z_open_ended_2_num_words',\n",
    " 'Z_open_ended_2_num_misspelled',\n",
    " 'Z_open_ended_2_percent_misspelled',\n",
    " 'Z_open_ended_3_num_words',\n",
    " 'Z_open_ended_3_num_misspelled',\n",
    " 'Z_open_ended_3_percent_misspelled',\n",
    " 'Z_open_ended_4_num_words',\n",
    " 'Z_open_ended_4_num_misspelled',\n",
    " 'Z_open_ended_4_percent_misspelled',\n",
    " 'Z_open_ended_5_num_words',\n",
    " 'Z_open_ended_5_num_misspelled',\n",
    " 'Z_open_ended_5_percent_misspelled',\n",
    " 'Z_open_ended_6_num_words',\n",
    " 'Z_open_ended_6_num_misspelled',\n",
    " 'Z_open_ended_6_percent_misspelled',\n",
    " 'Z_O_high_5',\n",
    " 'Z_C_high_2',\n",
    " 'Z_A_high_1',\n",
    " 'Z_E_high_3',\n",
    " 'Z_A_low_2',\n",
    " 'Z_A_low_3',\n",
    " 'Z_A_low_4',\n",
    " 'Z_A_low_5',\n",
    " 'Z_N_low_1',\n",
    " 'Z_N_low_2',\n",
    " 'Z_N_low_3',\n",
    " 'Z_N_low_5',\n",
    " 'Z_C_high_1',\n",
    " 'Z_C_high_3',\n",
    " 'Z_C_high_4',\n",
    " 'Z_C_high_5',\n",
    " 'Z_A_high_2',\n",
    " 'Z_A_high_3',\n",
    " 'Z_A_high_4',\n",
    " 'Z_A_high_5',\n",
    " 'Z_N_high_1',\n",
    " 'Z_N_high_2',\n",
    " 'Z_N_high_3',\n",
    " 'Z_N_high_5',\n",
    " 'Z_O_high_1',\n",
    " 'Z_O_high_2',\n",
    " 'Z_O_high_3',\n",
    " 'Z_O_high_4',\n",
    " 'Z_E_high_4',\n",
    " 'Z_E_high_5',\n",
    " 'Z_A_low_1',\n",
    " 'Z_E_low_3',\n",
    " 'Z_GO_3',\n",
    " 'Z_NOGO_3',\n",
    " 'Z_GO_5',\n",
    " 'Z_NOGO_5',\n",
    " 'Z_NOT_1',\n",
    " 'Z_NOT_2',\n",
    " 'Z_NOT_3',\n",
    " 'Z_NOT_4',\n",
    " 'Z_NOT_5',\n",
    " 'Z_NO_5',\n",
    " 'Z_A_low_comb',\n",
    " 'Z_N_low_comb',\n",
    " 'Z_C_high_comb',\n",
    " 'Z_A_high_comb',\n",
    " 'Z_N_high_comb',\n",
    " 'Z_O_high_comb',\n",
    " 'Z_E_high_3to5',\n",
    " 'Z_A_not_comb',\n",
    " 'Z_O_go_comb',\n",
    " 'Z_open_ended_1_num_chars',\n",
    " 'Z_open_ended_1_flesch_grade',\n",
    " 'Z_open_ended_2_num_chars',\n",
    " 'Z_open_ended_2_flesch_grade',\n",
    " 'Z_open_ended_3_num_chars',\n",
    " 'Z_open_ended_3_flesch_grade',\n",
    " 'Z_open_ended_4_num_chars',\n",
    " 'Z_open_ended_4_flesch_grade',\n",
    " 'Z_open_ended_5_num_chars',\n",
    " 'Z_open_ended_5_flesch_grade',\n",
    " 'Z_open_ended_6_num_chars',\n",
    " 'Z_open_ended_6_flesch_grade',\n",
    " 'Z_readability_syllables_count',\n",
    " 'Z_readability_word_count',\n",
    " 'Z_readability_difficult_count',\n",
    " 'Z_readability_sentence_count',\n",
    " 'Z_readability_avg_syllables_per_word',\n",
    " 'Z_readability_avg_sentence_length',\n",
    " 'Z_readability_flesch_ease_score',\n",
    " 'Z_readability_flesch_grade_score',\n",
    " 'Z_readability_linsear_write_score',\n",
    " 'Z_readability_dale_chall_score',\n",
    " 'Z_readability_smog_score',\n",
    " 'Z_readability_coleman_liau_score',\n",
    " 'Z_number_grammar_errors',\n",
    " 'Z_Avg_word_length_1',\n",
    " 'Z_Avg_word_length_2',\n",
    " 'Z_Avg_word_length_3',\n",
    " 'Z_Avg_word_length_4',\n",
    " 'Z_Avg_word_length_5',\n",
    " 'Z_Avg_word_length_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z_open_ended_1_num_words',\n",
       " 'Z_open_ended_1_num_misspelled',\n",
       " 'Z_open_ended_1_percent_misspelled',\n",
       " 'Z_open_ended_2_num_words',\n",
       " 'Z_open_ended_2_num_misspelled',\n",
       " 'Z_open_ended_2_percent_misspelled',\n",
       " 'Z_open_ended_3_num_words',\n",
       " 'Z_open_ended_3_num_misspelled',\n",
       " 'Z_open_ended_3_percent_misspelled',\n",
       " 'Z_open_ended_4_num_words',\n",
       " 'Z_open_ended_4_num_misspelled',\n",
       " 'Z_open_ended_4_percent_misspelled',\n",
       " 'Z_open_ended_5_num_words',\n",
       " 'Z_open_ended_5_num_misspelled',\n",
       " 'Z_open_ended_5_percent_misspelled',\n",
       " 'Z_open_ended_6_num_words',\n",
       " 'Z_open_ended_6_num_misspelled',\n",
       " 'Z_open_ended_6_percent_misspelled',\n",
       " 'Z_O_high_5',\n",
       " 'Z_C_high_2',\n",
       " 'Z_A_high_1',\n",
       " 'Z_E_high_3',\n",
       " 'Z_A_low_2',\n",
       " 'Z_A_low_3',\n",
       " 'Z_A_low_4',\n",
       " 'Z_A_low_5',\n",
       " 'Z_N_low_1',\n",
       " 'Z_N_low_2',\n",
       " 'Z_N_low_3',\n",
       " 'Z_N_low_5',\n",
       " 'Z_C_high_1',\n",
       " 'Z_C_high_3',\n",
       " 'Z_C_high_4',\n",
       " 'Z_C_high_5',\n",
       " 'Z_A_high_2',\n",
       " 'Z_A_high_3',\n",
       " 'Z_A_high_4',\n",
       " 'Z_A_high_5',\n",
       " 'Z_N_high_1',\n",
       " 'Z_N_high_2',\n",
       " 'Z_N_high_3',\n",
       " 'Z_N_high_5',\n",
       " 'Z_O_high_1',\n",
       " 'Z_O_high_2',\n",
       " 'Z_O_high_3',\n",
       " 'Z_O_high_4',\n",
       " 'Z_E_high_4',\n",
       " 'Z_E_high_5',\n",
       " 'Z_A_low_1',\n",
       " 'Z_E_low_3',\n",
       " 'Z_GO_3',\n",
       " 'Z_NOGO_3',\n",
       " 'Z_GO_5',\n",
       " 'Z_NOGO_5',\n",
       " 'Z_NOT_1',\n",
       " 'Z_NOT_2',\n",
       " 'Z_NOT_3',\n",
       " 'Z_NOT_4',\n",
       " 'Z_NOT_5',\n",
       " 'Z_NO_5',\n",
       " 'Z_A_low_comb',\n",
       " 'Z_N_low_comb',\n",
       " 'Z_C_high_comb',\n",
       " 'Z_A_high_comb',\n",
       " 'Z_N_high_comb',\n",
       " 'Z_O_high_comb',\n",
       " 'Z_E_high_3to5',\n",
       " 'Z_A_not_comb',\n",
       " 'Z_O_go_comb',\n",
       " 'Z_open_ended_1_num_chars',\n",
       " 'Z_open_ended_1_flesch_grade',\n",
       " 'Z_open_ended_2_num_chars',\n",
       " 'Z_open_ended_2_flesch_grade',\n",
       " 'Z_open_ended_3_num_chars',\n",
       " 'Z_open_ended_3_flesch_grade',\n",
       " 'Z_open_ended_4_num_chars',\n",
       " 'Z_open_ended_4_flesch_grade',\n",
       " 'Z_open_ended_5_num_chars',\n",
       " 'Z_open_ended_5_flesch_grade',\n",
       " 'Z_open_ended_6_num_chars',\n",
       " 'Z_open_ended_6_flesch_grade',\n",
       " 'Z_readability_syllables_count',\n",
       " 'Z_readability_word_count',\n",
       " 'Z_readability_difficult_count',\n",
       " 'Z_readability_sentence_count',\n",
       " 'Z_readability_avg_syllables_per_word',\n",
       " 'Z_readability_avg_sentence_length',\n",
       " 'Z_readability_flesch_ease_score',\n",
       " 'Z_readability_flesch_grade_score',\n",
       " 'Z_readability_linsear_write_score',\n",
       " 'Z_readability_dale_chall_score',\n",
       " 'Z_readability_smog_score',\n",
       " 'Z_readability_coleman_liau_score',\n",
       " 'Z_number_grammar_errors',\n",
       " 'Z_Avg_word_length_1',\n",
       " 'Z_Avg_word_length_2',\n",
       " 'Z_Avg_word_length_3',\n",
       " 'Z_Avg_word_length_4',\n",
       " 'Z_Avg_word_length_5',\n",
       " 'Z_Avg_word_length_6']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_Var_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset dataframes. \n",
    "\n",
    "df_train=df_total.loc[df_total['Source']=='Train'] \n",
    "df_test=df_total.loc[df_total['Source']=='Test'] \n",
    "df_final=df_total.loc[df_total['Source']=='Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source\n",
       "Train    1088\n",
       "Test      300\n",
       "Final     300\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total['Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                              \n",
      "Generation 1 - Current best internal CV score: -0.45993245043907355\n",
      "                                                                             \n",
      "Generation 2 - Current best internal CV score: -0.45993245043907355\n",
      "                                                                             \n",
      "Generation 3 - Current best internal CV score: -0.459039883890072\n",
      "                                                                             \n",
      "Generation 4 - Current best internal CV score: -0.45555577086707466\n",
      "                                                                              \n",
      "Generation 5 - Current best internal CV score: -0.45378904790778307\n",
      "                                                                              \n",
      "Generation 6 - Current best internal CV score: -0.45378904790778307\n",
      "                                                                                \n",
      "Generation 7 - Current best internal CV score: -0.45378904790778307\n",
      "                                                                              \n",
      "Generation 8 - Current best internal CV score: -0.45370598804432516\n",
      "                                                                              \n",
      "Generation 9 - Current best internal CV score: -0.45370598804432516\n",
      "                                                                              \n",
      "Generation 10 - Current best internal CV score: -0.45370598804432516\n",
      "                                                                              \n",
      "Generation 11 - Current best internal CV score: -0.45370598804432516\n",
      "                                                                              \n",
      "Generation 12 - Current best internal CV score: -0.45238928817116014\n",
      "                                                                              \n",
      "Generation 13 - Current best internal CV score: -0.45238928817116014\n",
      "                                                                              \n",
      "Generation 14 - Current best internal CV score: -0.45238928817116014\n",
      "                                                                              \n",
      "Generation 15 - Current best internal CV score: -0.45238928817116014\n",
      "                                                                              \n",
      "Generation 16 - Current best internal CV score: -0.45238928817116014\n",
      "                                                                              \n",
      "Generation 17 - Current best internal CV score: -0.45238928817116014\n",
      "                                                                              \n",
      "Generation 18 - Current best internal CV score: -0.45238928817116014\n",
      "                                                                              \n",
      "Generation 19 - Current best internal CV score: -0.45238928817116014\n",
      "                                                                              \n",
      "Generation 20 - Current best internal CV score: -0.45238928817116014\n",
      "                                                                              \n",
      "Best pipeline: ExtraTreesRegressor(LinearSVR(StandardScaler(input_matrix), C=5.0, dual=False, epsilon=0.001, loss=squared_epsilon_insensitive, tol=0.01), bootstrap=True, max_features=0.35000000000000003, min_samples_leaf=19, min_samples_split=9, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-7 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-7 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-7 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-7 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-7 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-7 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TPOTRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['O_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "O_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2, n_jobs = -1)\n",
    "O_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                             \n",
      "Generation 1 - Current best internal CV score: -0.2820844364049678\n",
      "                                                                             \n",
      "Generation 2 - Current best internal CV score: -0.2820348344037934\n",
      "                                                                             \n",
      "Generation 3 - Current best internal CV score: -0.2820348344037934\n",
      "                                                                             \n",
      "Generation 4 - Current best internal CV score: -0.2820348344037934\n",
      "                                                                              \n",
      "Generation 5 - Current best internal CV score: -0.2820348344037934\n",
      "                                                                              \n",
      "Generation 6 - Current best internal CV score: -0.28104864293570725\n",
      "                                                                              \n",
      "Generation 7 - Current best internal CV score: -0.2809814557860172\n",
      "                                                                              \n",
      "Generation 8 - Current best internal CV score: -0.2809814557860172\n",
      "                                                                              \n",
      "Generation 9 - Current best internal CV score: -0.2809606680349291\n",
      "                                                                              \n",
      "Generation 10 - Current best internal CV score: -0.2809448724742617\n",
      "                                                                              \n",
      "Generation 11 - Current best internal CV score: -0.2785426725147371\n",
      "                                                                              \n",
      "Generation 12 - Current best internal CV score: -0.2784913083350621\n",
      "                                                                              \n",
      "Generation 13 - Current best internal CV score: -0.2784913083350621\n",
      "                                                                              \n",
      "Generation 14 - Current best internal CV score: -0.2784913083350621\n",
      "                                                                              \n",
      "Generation 15 - Current best internal CV score: -0.2784913083350621\n",
      "                                                                              \n",
      "Generation 16 - Current best internal CV score: -0.27785688998391705\n",
      "                                                                              \n",
      "Generation 17 - Current best internal CV score: -0.2778297067831097\n",
      "                                                                              \n",
      "Generation 18 - Current best internal CV score: -0.2778297067831097\n",
      "                                                                              \n",
      "Generation 19 - Current best internal CV score: -0.2778297067831097\n",
      "                                                                              \n",
      "Generation 20 - Current best internal CV score: -0.27777891865617377\n",
      "                                                                              \n",
      "Best pipeline: RandomForestRegressor(ElasticNetCV(input_matrix, l1_ratio=0.6000000000000001, tol=0.0001), bootstrap=True, max_features=0.35000000000000003, min_samples_leaf=16, min_samples_split=14, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-8 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-8 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-8 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-8 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-8 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-8 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TPOTRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['C_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "C_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2, n_jobs = -1)\n",
    "C_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                             \n",
      "Generation 1 - Current best internal CV score: -0.542754539535164\n",
      "                                                                             \n",
      "Generation 2 - Current best internal CV score: -0.5411311434691033\n",
      "                                                                             \n",
      "Generation 3 - Current best internal CV score: -0.5356820304161364\n",
      "                                                                             \n",
      "Generation 4 - Current best internal CV score: -0.5316581617030952\n",
      "                                                                              \n",
      "Generation 5 - Current best internal CV score: -0.5316200616199164\n",
      "                                                                              \n",
      "Generation 6 - Current best internal CV score: -0.5316200616199164\n",
      "                                                                              \n",
      "Generation 7 - Current best internal CV score: -0.5314136279341382\n",
      "                                                                              \n",
      "Generation 8 - Current best internal CV score: -0.5314136279341382\n",
      "                                                                              \n",
      "Generation 9 - Current best internal CV score: -0.5314136279341382\n",
      "                                                                              \n",
      "Generation 10 - Current best internal CV score: -0.5314136279341382\n",
      "                                                                              \n",
      "Generation 11 - Current best internal CV score: -0.5314136279341382\n",
      "                                                                              \n",
      "Generation 12 - Current best internal CV score: -0.5314136279341382\n",
      "                                                                              \n",
      "Generation 13 - Current best internal CV score: -0.5314136279341382\n",
      "                                                                              \n",
      "Generation 14 - Current best internal CV score: -0.5310370040199254\n",
      "                                                                              \n",
      "Generation 15 - Current best internal CV score: -0.5299228518441691\n",
      "                                                                              \n",
      "Generation 16 - Current best internal CV score: -0.5299228518441691\n",
      "                                                                              \n",
      "Generation 17 - Current best internal CV score: -0.5299228518441691\n",
      "                                                                              \n",
      "Generation 18 - Current best internal CV score: -0.5299228518441691\n",
      "                                                                              \n",
      "Generation 19 - Current best internal CV score: -0.5299228518441691\n",
      "                                                                              \n",
      "Generation 20 - Current best internal CV score: -0.5299228518441691\n",
      "                                                                              \n",
      "Best pipeline: RandomForestRegressor(MinMaxScaler(ElasticNetCV(input_matrix, l1_ratio=0.75, tol=0.01)), bootstrap=True, max_features=0.9500000000000001, min_samples_leaf=16, min_samples_split=14, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-9 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-9 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-9 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-9 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-9 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TPOTRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['E_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "E_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2, n_jobs = -1)\n",
    "E_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Generation 1 - Current best internal CV score: -0.29736157592028345\n",
      "                                                                             \n",
      "Generation 2 - Current best internal CV score: -0.29736157592028345\n",
      "                                                                             \n",
      "Generation 3 - Current best internal CV score: -0.29732741318582306\n",
      "                                                                              \n",
      "Generation 4 - Current best internal CV score: -0.29732741318582306\n",
      "                                                                              \n",
      "Generation 5 - Current best internal CV score: -0.29732741318582306\n",
      "                                                                              \n",
      "Generation 6 - Current best internal CV score: -0.29637548798815383\n",
      "                                                                              \n",
      "Generation 7 - Current best internal CV score: -0.29637548798815383\n",
      "                                                                              \n",
      "Generation 8 - Current best internal CV score: -0.29637548798815383\n",
      "                                                                              \n",
      "Generation 9 - Current best internal CV score: -0.2949186462725845\n",
      "                                                                              \n",
      "Generation 10 - Current best internal CV score: -0.2949186462725845\n",
      "                                                                              \n",
      "Generation 11 - Current best internal CV score: -0.2949186462725845\n",
      "                                                                              \n",
      "Generation 12 - Current best internal CV score: -0.2949186462725845\n",
      "                                                                              \n",
      "Generation 13 - Current best internal CV score: -0.29381240607540193\n",
      "                                                                              \n",
      "Generation 14 - Current best internal CV score: -0.29381240607540193\n",
      "                                                                              \n",
      "Generation 15 - Current best internal CV score: -0.29381240607540193\n",
      "                                                                              \n",
      "Generation 16 - Current best internal CV score: -0.29381240607540193\n",
      "                                                                              \n",
      "Generation 17 - Current best internal CV score: -0.29381240607540193\n",
      "                                                                              \n",
      "Generation 18 - Current best internal CV score: -0.29381240607540193\n",
      "                                                                              \n",
      "Generation 19 - Current best internal CV score: -0.29381240607540193\n",
      "                                                                              \n",
      "Generation 20 - Current best internal CV score: -0.29381240607540193\n",
      "                                                           \n",
      "Best pipeline: RandomForestRegressor(ElasticNetCV(input_matrix, l1_ratio=0.75, tol=0.1), bootstrap=True, max_features=0.6000000000000001, min_samples_leaf=3, min_samples_split=14, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-10 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-10 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-10 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-10 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-10 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TPOTRegressor(generations=20, population_size=20, random_state=42, verbosity=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TPOTRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TPOTRegressor(generations=20, population_size=20, random_state=42, verbosity=2)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TPOTRegressor(generations=20, population_size=20, random_state=42, verbosity=2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['A_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "A_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2)\n",
    "A_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                              \n",
      "Generation 1 - Current best internal CV score: -0.48104831075238186\n",
      "                                                                             \n",
      "Generation 2 - Current best internal CV score: -0.48104831075238186\n",
      "                                                                             \n",
      "Generation 3 - Current best internal CV score: -0.47922569308107477\n",
      "                                                                             \n",
      "Generation 4 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 5 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 6 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 7 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 8 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 9 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 10 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 11 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 12 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 13 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 14 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 15 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 16 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 17 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 18 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 19 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Generation 20 - Current best internal CV score: -0.47861565312888354\n",
      "                                                                              \n",
      "Best pipeline: ElasticNetCV(input_matrix, l1_ratio=1.0, tol=0.001)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-11 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-11 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-11 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-11 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-11 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-11 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-11 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-11 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-11 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-11 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-11 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-11 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TPOTRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TPOTRegressor(generations=20, n_jobs=-1, population_size=20, random_state=42,\n",
       "              verbosity=2)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto ML with feature set predicting Agreeableness\n",
    "X = df_train[Z_Var_List]\n",
    "Y = df_train['N_Scale_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.60, test_size=0.40)\n",
    "N_pipeline_optimizer = TPOTRegressor(generations=20, population_size=20, cv=5,random_state=42, verbosity=2, n_jobs = -1)\n",
    "N_pipeline_optimizer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z_open_ended_1_num_words',\n",
       " 'Z_open_ended_1_num_misspelled',\n",
       " 'Z_open_ended_1_percent_misspelled',\n",
       " 'Z_open_ended_2_num_words',\n",
       " 'Z_open_ended_2_num_misspelled',\n",
       " 'Z_open_ended_2_percent_misspelled',\n",
       " 'Z_open_ended_3_num_words',\n",
       " 'Z_open_ended_3_num_misspelled',\n",
       " 'Z_open_ended_3_percent_misspelled',\n",
       " 'Z_open_ended_4_num_words',\n",
       " 'Z_open_ended_4_num_misspelled',\n",
       " 'Z_open_ended_4_percent_misspelled',\n",
       " 'Z_open_ended_5_num_words',\n",
       " 'Z_open_ended_5_num_misspelled',\n",
       " 'Z_open_ended_5_percent_misspelled',\n",
       " 'Z_open_ended_6_num_words',\n",
       " 'Z_open_ended_6_num_misspelled',\n",
       " 'Z_open_ended_6_percent_misspelled',\n",
       " 'Z_O_high_5',\n",
       " 'Z_C_high_2',\n",
       " 'Z_A_high_1',\n",
       " 'Z_E_high_3',\n",
       " 'Z_A_low_2',\n",
       " 'Z_A_low_3',\n",
       " 'Z_A_low_4',\n",
       " 'Z_A_low_5',\n",
       " 'Z_N_low_1',\n",
       " 'Z_N_low_2',\n",
       " 'Z_N_low_3',\n",
       " 'Z_N_low_5',\n",
       " 'Z_C_high_1',\n",
       " 'Z_C_high_3',\n",
       " 'Z_C_high_4',\n",
       " 'Z_C_high_5',\n",
       " 'Z_A_high_2',\n",
       " 'Z_A_high_3',\n",
       " 'Z_A_high_4',\n",
       " 'Z_A_high_5',\n",
       " 'Z_N_high_1',\n",
       " 'Z_N_high_2',\n",
       " 'Z_N_high_3',\n",
       " 'Z_N_high_5',\n",
       " 'Z_O_high_1',\n",
       " 'Z_O_high_2',\n",
       " 'Z_O_high_3',\n",
       " 'Z_O_high_4',\n",
       " 'Z_E_high_4',\n",
       " 'Z_E_high_5',\n",
       " 'Z_A_low_1',\n",
       " 'Z_E_low_3',\n",
       " 'Z_GO_3',\n",
       " 'Z_NOGO_3',\n",
       " 'Z_GO_5',\n",
       " 'Z_NOGO_5',\n",
       " 'Z_NOT_1',\n",
       " 'Z_NOT_2',\n",
       " 'Z_NOT_3',\n",
       " 'Z_NOT_4',\n",
       " 'Z_NOT_5',\n",
       " 'Z_NO_5',\n",
       " 'Z_A_low_comb',\n",
       " 'Z_N_low_comb',\n",
       " 'Z_C_high_comb',\n",
       " 'Z_A_high_comb',\n",
       " 'Z_N_high_comb',\n",
       " 'Z_O_high_comb',\n",
       " 'Z_E_high_3to5',\n",
       " 'Z_A_not_comb',\n",
       " 'Z_O_go_comb',\n",
       " 'Z_open_ended_1_num_chars',\n",
       " 'Z_open_ended_1_flesch_grade',\n",
       " 'Z_open_ended_2_num_chars',\n",
       " 'Z_open_ended_2_flesch_grade',\n",
       " 'Z_open_ended_3_num_chars',\n",
       " 'Z_open_ended_3_flesch_grade',\n",
       " 'Z_open_ended_4_num_chars',\n",
       " 'Z_open_ended_4_flesch_grade',\n",
       " 'Z_open_ended_5_num_chars',\n",
       " 'Z_open_ended_5_flesch_grade',\n",
       " 'Z_open_ended_6_num_chars',\n",
       " 'Z_open_ended_6_flesch_grade',\n",
       " 'Z_readability_syllables_count',\n",
       " 'Z_readability_word_count',\n",
       " 'Z_readability_difficult_count',\n",
       " 'Z_readability_sentence_count',\n",
       " 'Z_readability_avg_syllables_per_word',\n",
       " 'Z_readability_avg_sentence_length',\n",
       " 'Z_readability_flesch_ease_score',\n",
       " 'Z_readability_flesch_grade_score',\n",
       " 'Z_readability_linsear_write_score',\n",
       " 'Z_readability_dale_chall_score',\n",
       " 'Z_readability_smog_score',\n",
       " 'Z_readability_coleman_liau_score',\n",
       " 'Z_number_grammar_errors',\n",
       " 'Z_Avg_word_length_1',\n",
       " 'Z_Avg_word_length_2',\n",
       " 'Z_Avg_word_length_3',\n",
       " 'Z_Avg_word_length_4',\n",
       " 'Z_Avg_word_length_5',\n",
       " 'Z_Avg_word_length_6']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_Var_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_Scale_score</th>\n",
       "      <th>C_Scale_score</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>E_Scale_score</th>\n",
       "      <th>N_Scale_score</th>\n",
       "      <th>O_Scale_score</th>\n",
       "      <th>Respondent_ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>open_ended_1</th>\n",
       "      <th>open_ended_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Z_readability_dale_chall_score</th>\n",
       "      <th>Z_readability_smog_score</th>\n",
       "      <th>Z_readability_coleman_liau_score</th>\n",
       "      <th>Z_number_grammar_errors</th>\n",
       "      <th>Z_Avg_word_length_1</th>\n",
       "      <th>Z_Avg_word_length_2</th>\n",
       "      <th>Z_Avg_word_length_3</th>\n",
       "      <th>Z_Avg_word_length_4</th>\n",
       "      <th>Z_Avg_word_length_5</th>\n",
       "      <th>Z_Avg_word_length_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>Dev</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>10460010474</td>\n",
       "      <td>Test</td>\n",
       "      <td>I would look into changing my vacation plans t...</td>\n",
       "      <td>I would work on the project little by little d...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.601579</td>\n",
       "      <td>-0.248236</td>\n",
       "      <td>-0.050278</td>\n",
       "      <td>-0.465280</td>\n",
       "      <td>0.547747</td>\n",
       "      <td>-0.220849</td>\n",
       "      <td>-0.374477</td>\n",
       "      <td>-0.216930</td>\n",
       "      <td>1.372646</td>\n",
       "      <td>0.082688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>4.583333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Dev</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>10440103178</td>\n",
       "      <td>Test</td>\n",
       "      <td>I have always been a team player, but this wou...</td>\n",
       "      <td>I would first address my concerns with my boss...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044436</td>\n",
       "      <td>-0.416939</td>\n",
       "      <td>-0.122402</td>\n",
       "      <td>1.870369</td>\n",
       "      <td>-0.254109</td>\n",
       "      <td>1.056723</td>\n",
       "      <td>-0.501801</td>\n",
       "      <td>0.907520</td>\n",
       "      <td>-0.446863</td>\n",
       "      <td>0.316143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>Dev</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>10440099430</td>\n",
       "      <td>Test</td>\n",
       "      <td>I would try to come to a compromise with my co...</td>\n",
       "      <td>I would go to my boss and ask him if he has an...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347627</td>\n",
       "      <td>0.595275</td>\n",
       "      <td>-0.253536</td>\n",
       "      <td>-1.079924</td>\n",
       "      <td>-0.313317</td>\n",
       "      <td>-1.927906</td>\n",
       "      <td>-1.610265</td>\n",
       "      <td>-0.069316</td>\n",
       "      <td>0.409290</td>\n",
       "      <td>-1.161596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>Dev</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.916667</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>10460189074</td>\n",
       "      <td>Test</td>\n",
       "      <td>I would explain to the supervisor why I need t...</td>\n",
       "      <td>I would try to finish the project as soon as I...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.711632</td>\n",
       "      <td>-0.653122</td>\n",
       "      <td>-1.040341</td>\n",
       "      <td>-0.956996</td>\n",
       "      <td>-0.208275</td>\n",
       "      <td>-0.359690</td>\n",
       "      <td>-1.356688</td>\n",
       "      <td>-0.375642</td>\n",
       "      <td>-0.573661</td>\n",
       "      <td>-0.808626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>Dev</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>10459700329</td>\n",
       "      <td>Test</td>\n",
       "      <td>I would tell them I will work this time if nex...</td>\n",
       "      <td>I would let him know I have a big project and ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943394</td>\n",
       "      <td>-3.554802</td>\n",
       "      <td>-0.174856</td>\n",
       "      <td>-2.678000</td>\n",
       "      <td>-2.676775</td>\n",
       "      <td>-2.248792</td>\n",
       "      <td>0.973655</td>\n",
       "      <td>-1.616769</td>\n",
       "      <td>-0.373999</td>\n",
       "      <td>-1.822368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>Dev</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>10460095413</td>\n",
       "      <td>Test</td>\n",
       "      <td>I would avoid doing anything. It's likely I pu...</td>\n",
       "      <td>I would plan out how long it would take for me...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188880</td>\n",
       "      <td>-0.518160</td>\n",
       "      <td>-0.915764</td>\n",
       "      <td>0.641080</td>\n",
       "      <td>-0.234286</td>\n",
       "      <td>-0.320575</td>\n",
       "      <td>-0.979713</td>\n",
       "      <td>-0.600724</td>\n",
       "      <td>-0.468943</td>\n",
       "      <td>-0.916312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>4.833333</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>Dev</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>10459955025</td>\n",
       "      <td>Test</td>\n",
       "      <td>If my colleague was not willing to change thei...</td>\n",
       "      <td>I would complete the project the earliest poss...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.360838</td>\n",
       "      <td>-0.450679</td>\n",
       "      <td>-0.469908</td>\n",
       "      <td>0.026436</td>\n",
       "      <td>0.365695</td>\n",
       "      <td>0.164579</td>\n",
       "      <td>-1.362333</td>\n",
       "      <td>0.655415</td>\n",
       "      <td>0.197926</td>\n",
       "      <td>0.109735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Dev</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10460413642</td>\n",
       "      <td>Test</td>\n",
       "      <td>I would start by finding out if they have plan...</td>\n",
       "      <td>I would complete my project with plenty of tim...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209515</td>\n",
       "      <td>-0.147015</td>\n",
       "      <td>0.107083</td>\n",
       "      <td>0.886938</td>\n",
       "      <td>1.434860</td>\n",
       "      <td>-0.049981</td>\n",
       "      <td>0.398559</td>\n",
       "      <td>-0.279103</td>\n",
       "      <td>-0.896192</td>\n",
       "      <td>0.179590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>Dev</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>10460105559</td>\n",
       "      <td>Test</td>\n",
       "      <td>I would ask my supervisor that since I am not ...</td>\n",
       "      <td>I would get this project done and over with.  ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189426</td>\n",
       "      <td>0.089168</td>\n",
       "      <td>-0.181413</td>\n",
       "      <td>-0.219422</td>\n",
       "      <td>-0.493101</td>\n",
       "      <td>-0.435618</td>\n",
       "      <td>0.474347</td>\n",
       "      <td>-0.950852</td>\n",
       "      <td>0.271063</td>\n",
       "      <td>-0.476115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>4.916667</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>Dev</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>10440097909</td>\n",
       "      <td>Test</td>\n",
       "      <td>I would see if I was able to change my vacatio...</td>\n",
       "      <td>I would begin by working on the project as soo...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340203</td>\n",
       "      <td>-0.315717</td>\n",
       "      <td>-0.345330</td>\n",
       "      <td>-0.588209</td>\n",
       "      <td>-0.930412</td>\n",
       "      <td>0.295380</td>\n",
       "      <td>-1.348503</td>\n",
       "      <td>-0.531807</td>\n",
       "      <td>1.263146</td>\n",
       "      <td>-0.523917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 408 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A_Scale_score  C_Scale_score Dataset  E_Scale_score  N_Scale_score  \\\n",
       "1088       4.666667       4.583333     Dev       3.833333       1.583333   \n",
       "1089       4.583333       5.000000     Dev       4.083333       3.166667   \n",
       "1090       4.166667       4.000000     Dev       2.500000       1.750000   \n",
       "1091       4.166667       4.750000     Dev       4.500000       1.916667   \n",
       "1092       3.666667       4.000000     Dev       3.833333       2.333333   \n",
       "...             ...            ...     ...            ...            ...   \n",
       "1383       2.500000       4.416667     Dev       2.666667       1.333333   \n",
       "1384       4.833333       4.916667     Dev       4.416667       1.083333   \n",
       "1385       4.750000       5.000000     Dev       3.833333       1.166667   \n",
       "1386       4.333333       4.000000     Dev       3.750000       2.166667   \n",
       "1387       4.916667       4.250000     Dev       3.166667       2.250000   \n",
       "\n",
       "      O_Scale_score  Respondent_ID Source  \\\n",
       "1088       4.416667    10460010474   Test   \n",
       "1089       4.750000    10440103178   Test   \n",
       "1090       2.416667    10440099430   Test   \n",
       "1091       4.500000    10460189074   Test   \n",
       "1092       3.833333    10459700329   Test   \n",
       "...             ...            ...    ...   \n",
       "1383       4.083333    10460095413   Test   \n",
       "1384       4.583333    10459955025   Test   \n",
       "1385       4.000000    10460413642   Test   \n",
       "1386       3.750000    10460105559   Test   \n",
       "1387       4.583333    10440097909   Test   \n",
       "\n",
       "                                           open_ended_1  \\\n",
       "1088  I would look into changing my vacation plans t...   \n",
       "1089  I have always been a team player, but this wou...   \n",
       "1090  I would try to come to a compromise with my co...   \n",
       "1091  I would explain to the supervisor why I need t...   \n",
       "1092  I would tell them I will work this time if nex...   \n",
       "...                                                 ...   \n",
       "1383  I would avoid doing anything. It's likely I pu...   \n",
       "1384  If my colleague was not willing to change thei...   \n",
       "1385  I would start by finding out if they have plan...   \n",
       "1386  I would ask my supervisor that since I am not ...   \n",
       "1387  I would see if I was able to change my vacatio...   \n",
       "\n",
       "                                           open_ended_2  ...  \\\n",
       "1088  I would work on the project little by little d...  ...   \n",
       "1089  I would first address my concerns with my boss...  ...   \n",
       "1090  I would go to my boss and ask him if he has an...  ...   \n",
       "1091  I would try to finish the project as soon as I...  ...   \n",
       "1092  I would let him know I have a big project and ...  ...   \n",
       "...                                                 ...  ...   \n",
       "1383  I would plan out how long it would take for me...  ...   \n",
       "1384  I would complete the project the earliest poss...  ...   \n",
       "1385  I would complete my project with plenty of tim...  ...   \n",
       "1386  I would get this project done and over with.  ...  ...   \n",
       "1387  I would begin by working on the project as soo...  ...   \n",
       "\n",
       "     Z_readability_dale_chall_score Z_readability_smog_score  \\\n",
       "1088                      -0.601579                -0.248236   \n",
       "1089                      -0.044436                -0.416939   \n",
       "1090                       0.347627                 0.595275   \n",
       "1091                      -0.711632                -0.653122   \n",
       "1092                       1.943394                -3.554802   \n",
       "...                             ...                      ...   \n",
       "1383                      -0.188880                -0.518160   \n",
       "1384                      -0.360838                -0.450679   \n",
       "1385                      -0.209515                -0.147015   \n",
       "1386                       0.189426                 0.089168   \n",
       "1387                      -0.340203                -0.315717   \n",
       "\n",
       "     Z_readability_coleman_liau_score Z_number_grammar_errors  \\\n",
       "1088                        -0.050278               -0.465280   \n",
       "1089                        -0.122402                1.870369   \n",
       "1090                        -0.253536               -1.079924   \n",
       "1091                        -1.040341               -0.956996   \n",
       "1092                        -0.174856               -2.678000   \n",
       "...                               ...                     ...   \n",
       "1383                        -0.915764                0.641080   \n",
       "1384                        -0.469908                0.026436   \n",
       "1385                         0.107083                0.886938   \n",
       "1386                        -0.181413               -0.219422   \n",
       "1387                        -0.345330               -0.588209   \n",
       "\n",
       "      Z_Avg_word_length_1  Z_Avg_word_length_2  Z_Avg_word_length_3  \\\n",
       "1088             0.547747            -0.220849            -0.374477   \n",
       "1089            -0.254109             1.056723            -0.501801   \n",
       "1090            -0.313317            -1.927906            -1.610265   \n",
       "1091            -0.208275            -0.359690            -1.356688   \n",
       "1092            -2.676775            -2.248792             0.973655   \n",
       "...                   ...                  ...                  ...   \n",
       "1383            -0.234286            -0.320575            -0.979713   \n",
       "1384             0.365695             0.164579            -1.362333   \n",
       "1385             1.434860            -0.049981             0.398559   \n",
       "1386            -0.493101            -0.435618             0.474347   \n",
       "1387            -0.930412             0.295380            -1.348503   \n",
       "\n",
       "      Z_Avg_word_length_4  Z_Avg_word_length_5  Z_Avg_word_length_6  \n",
       "1088            -0.216930             1.372646             0.082688  \n",
       "1089             0.907520            -0.446863             0.316143  \n",
       "1090            -0.069316             0.409290            -1.161596  \n",
       "1091            -0.375642            -0.573661            -0.808626  \n",
       "1092            -1.616769            -0.373999            -1.822368  \n",
       "...                   ...                  ...                  ...  \n",
       "1383            -0.600724            -0.468943            -0.916312  \n",
       "1384             0.655415             0.197926             0.109735  \n",
       "1385            -0.279103            -0.896192             0.179590  \n",
       "1386            -0.950852             0.271063            -0.476115  \n",
       "1387            -0.531807             1.263146            -0.523917  \n",
       "\n",
       "[300 rows x 408 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.13700114, 3.98529496, 3.76850874, 3.81241361, 3.95560496,\n",
       "       4.10650083, 3.64369744, 4.12097779, 3.69105403, 3.91327018,\n",
       "       3.82800388, 3.86913752, 3.72453714, 4.13069866, 3.99793406,\n",
       "       3.93480533, 3.82143547, 3.83433478, 3.88208745, 4.13401051,\n",
       "       4.07582464, 3.65871354, 3.96590469, 4.00245045, 3.7130331 ,\n",
       "       4.07624827, 3.88403707, 3.7134188 , 3.92119074, 3.65718481,\n",
       "       3.80179018, 3.76754705, 3.85769076, 3.89545335, 3.82257067,\n",
       "       3.83625501, 3.99208356, 4.17172616, 4.19650189, 4.08437071,\n",
       "       3.91580636, 3.9714718 , 3.8360542 , 3.87428101, 3.85792152,\n",
       "       3.86125305, 3.82124171, 4.04952698, 3.63575167, 3.91672943,\n",
       "       4.04653174, 3.80335184, 3.8273403 , 3.80841406, 3.63404248,\n",
       "       3.83324893, 3.95141601, 4.14061789, 4.01209586, 4.0769265 ,\n",
       "       3.82025955, 3.87364002, 4.01114598, 3.60664977, 4.05031941,\n",
       "       3.91050771, 3.77805306, 3.93974303, 4.00936678, 4.00870947,\n",
       "       3.94334511, 3.92773253, 4.06169633, 3.86935908, 3.71830571,\n",
       "       3.67322044, 3.69309172, 3.75109718, 3.86426555, 3.88274024,\n",
       "       4.14938648, 3.84639727, 3.94908931, 4.16778967, 3.75862714,\n",
       "       4.04115421, 3.82522937, 4.04489008, 3.89384817, 3.8185778 ,\n",
       "       3.93925996, 3.95557695, 4.17787438, 4.01564929, 3.89131879,\n",
       "       4.09993045, 3.76809198, 4.11432344, 3.91036559, 3.88583496,\n",
       "       4.01227606, 3.53829841, 4.06261957, 3.95928336, 3.65177504,\n",
       "       3.6599267 , 3.66547536, 4.07443284, 3.79501787, 3.91361893,\n",
       "       4.03836501, 4.12236339, 3.85980053, 3.67045594, 3.82962601,\n",
       "       3.71355259, 4.16244405, 3.86824154, 4.11338648, 3.61121072,\n",
       "       3.65658208, 3.87754447, 3.58197541, 3.93416864, 3.863511  ,\n",
       "       3.84862782, 3.79935199, 3.90540867, 3.75509432, 3.82279629,\n",
       "       4.06183652, 3.97877607, 3.87378969, 3.64153067, 4.20552816,\n",
       "       3.47463697, 3.98364386, 3.64593898, 4.01115212, 3.68029971,\n",
       "       3.92407057, 4.03983716, 3.84683865, 3.75177454, 3.74328942,\n",
       "       3.62514403, 3.72224154, 3.74786521, 3.6534508 , 3.9481469 ,\n",
       "       4.14648571, 3.97243115, 3.66634931, 3.84245012, 4.03862169,\n",
       "       3.96578962, 3.91319245, 3.87943312, 3.98110354, 3.95152644,\n",
       "       3.99094347, 3.83854562, 3.79841149, 3.85463093, 4.11427413,\n",
       "       3.79497799, 3.86303838, 3.66915722, 3.61498856, 3.89086242,\n",
       "       3.90672965, 3.91256517, 3.95496783, 4.00636899, 3.89320787,\n",
       "       3.7703898 , 4.03172648, 3.76229345, 3.69243658, 3.89927084,\n",
       "       3.96021191, 3.98752718, 4.12343066, 3.94188528, 3.93631206,\n",
       "       4.00174453, 3.76651222, 4.10224636, 3.96086256, 3.8975677 ,\n",
       "       3.94259593, 3.66200037, 3.66030504, 3.80796803, 4.04187775,\n",
       "       4.11075733, 3.75629807, 3.91480758, 3.77329337, 3.65860356,\n",
       "       3.83926274, 4.12746744, 4.00124439, 4.07631533, 3.70054504,\n",
       "       3.82551522, 3.80052343, 3.73095423, 3.90442827, 3.79282167,\n",
       "       3.85061925, 3.93506064, 4.09135785, 3.84001014, 3.82607883,\n",
       "       3.63207667, 4.07086234, 4.03327628, 3.56285434, 3.87510152,\n",
       "       3.93458467, 3.60496578, 3.68868178, 3.95098068, 3.80404886,\n",
       "       3.93250461, 3.75768147, 3.95884726, 3.78980229, 3.93044343,\n",
       "       3.90699249, 4.07061007, 3.77022055, 4.10615568, 3.79580233,\n",
       "       3.81318584, 3.87304246, 4.02567977, 3.68613008, 3.79060278,\n",
       "       3.64751721, 3.99456202, 4.08502146, 3.67287945, 3.81912213,\n",
       "       4.04046999, 3.995362  , 3.90716624, 3.85758605, 3.86010113,\n",
       "       3.97306978, 4.06886714, 3.80547535, 3.96580347, 3.8450078 ,\n",
       "       3.87570167, 3.76820345, 4.06739822, 3.9768121 , 3.94004881,\n",
       "       3.83895673, 3.77647851, 3.87808749, 4.11478352, 3.8883795 ,\n",
       "       3.82732332, 3.87851691, 3.76075815, 4.06451087, 3.69257648,\n",
       "       3.6466368 , 3.46087374, 3.86451279, 3.84509316, 3.81254109,\n",
       "       4.00166964, 3.99191498, 3.76786267, 3.67982828, 3.79563059,\n",
       "       3.80512588, 3.84459108, 3.9733412 , 4.02193025, 3.78069676,\n",
       "       3.8521751 , 4.10910371, 3.87385457, 3.78254141, 3.90229608,\n",
       "       3.65217488, 4.10996122, 3.78858103, 3.82058027, 3.97174616,\n",
       "       3.67350843, 3.77137832, 3.98458358, 3.66101917, 3.69891005])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = O_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['O_Scale_pred'] = O_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['O_Scale_pred'] = O_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/Users/I745133/Desktop/git/NLP/venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but ElasticNetCV was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['C_Scale_pred'] = C_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['C_Scale_pred'] = C_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/Users/I745133/Desktop/git/NLP/venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but ElasticNetCV was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['E_Scale_pred'] = E_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['E_Scale_pred'] = E_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/Users/I745133/Desktop/git/NLP/venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but ElasticNetCV was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['A_Scale_pred'] = A_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['A_Scale_pred'] = A_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['N_Scale_pred'] = N_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['N_Scale_pred'] = N_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final['O_Scale_pred'] = O_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['O_Scale_pred'] = O_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
      "/Users/I745133/Desktop/git/NLP/venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but ElasticNetCV was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final['C_Scale_pred'] = C_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['C_Scale_pred'] = C_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
      "/Users/I745133/Desktop/git/NLP/venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but ElasticNetCV was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final['E_Scale_pred'] = E_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['E_Scale_pred'] = E_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
      "/Users/I745133/Desktop/git/NLP/venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but ElasticNetCV was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final['A_Scale_pred'] = A_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['A_Scale_pred'] = A_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final['N_Scale_pred'] = N_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
      "/var/folders/ty/czjlsz45117ck6pvljb7p19c0000gn/T/ipykernel_52560/3339117158.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['N_Scale_pred'] = N_pipeline_optimizer.predict(df_final[Z_Var_List])\n"
     ]
    }
   ],
   "source": [
    "# Save predicted values\n",
    "\n",
    "df_test['O_Scale_pred'] = O_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "df_test['C_Scale_pred'] = C_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "df_test['E_Scale_pred'] = E_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "df_test['A_Scale_pred'] = A_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "df_test['N_Scale_pred'] = N_pipeline_optimizer.predict(df_test[Z_Var_List])\n",
    "\n",
    "df_final['O_Scale_pred'] = O_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "df_final['C_Scale_pred'] = C_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "df_final['E_Scale_pred'] = E_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "df_final['A_Scale_pred'] = A_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "df_final['N_Scale_pred'] = N_pipeline_optimizer.predict(df_final[Z_Var_List])\n",
    "\n",
    "df_test.to_csv(\"ML Test.csv\", index = False)\n",
    "df_final.to_csv(\"ML Final.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Section\n",
    "\n",
    "We wrote this section to run independently from the previous sections (i.e., there are no independencies). In this way, you can see how we used deep learning without getting confused with all the other junk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 20:12:03.094276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-08 20:12:03.116885: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-08 20:12:03.116916: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 20:12:03.130853: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-08 20:12:03.889106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# from keras.layers import Dense, Dropout, Embedding, Flatten, Input, MaxPooling1D\n",
    "# from keras.optimizers import Adam, SGD\n",
    "# from keras.models import Sequential\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# # from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# from keras import backend as K \n",
    "# import keras.layers as layers\n",
    "# from keras.models import Model, load_model\n",
    "# from keras.engine import Layer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Initialize session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config=config)\n",
    "# K.set_session(sess)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow_hub import KerasLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure directory hierarchy aligns\n",
    "train_raw_df = pd.read_csv(\"/home/ubuntu/git/NLP/dissertation/data/train_rep.csv\")\n",
    "df_test = pd.read_csv(\"/home/ubuntu/git/NLP/dissertation/data/test_rep.csv\")\n",
    "df_dev = pd.read_csv(\"/home/ubuntu/git/NLP/dissertation/data/valid_rep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/NLP/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/git/NLP/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ubuntu/git/NLP/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/ubuntu/git/NLP/venv/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/home/ubuntu/git/NLP/venv/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for ELMo\n",
    "options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ELMo model\n",
    "elmo = Elmo(options_file, weight_file, num_output_representations=1, dropout=0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_dropout',\n",
       " '_elmo_lstm',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_has_cached_vocab',\n",
       " '_is_full_backward_hook',\n",
       " '_keep_sentence_boundaries',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_scalar_mixes',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_to_params',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'from_params',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_output_dim',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'scalar_mix_0',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'to_params',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(elmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elmo(\n",
       "  (_elmo_lstm): _ElmoBiLm(\n",
       "    (_token_embedder): _ElmoCharacterEncoder(\n",
       "      (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "      (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "      (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "      (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "      (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "      (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "      (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "      (_highways): Highway(\n",
       "        (_layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=2048, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    )\n",
       "    (_elmo_lstm): ElmoLstm(\n",
       "      (forward_layer_0): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "      (backward_layer_0): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "      (forward_layer_1): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "      (backward_layer_1): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_dropout): Dropout(p=0, inplace=False)\n",
       "  (scalar_mix_0): ScalarMix(\n",
       "    (scalar_parameters): ParameterList(\n",
       "        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have train_raw_df, df_test, and df_dev DataFrames\n",
    "\n",
    "ATTRIBUTE_LIST = [\"E\", \"A\", \"O\", \"C\", \"N\"]\n",
    "\n",
    "X = train_raw_df[['open_ended_' + str(idx) for idx in range(1, 6)]]\n",
    "Y = np.array(train_raw_df[[att + \"_Scale_score\" for att in ATTRIBUTE_LIST]].values)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.2,\n",
    "    random_state=23\n",
    ")\n",
    "\n",
    "# X_train = [X_train['open_ended_' + str(idx)].values for idx in range(1, 6)]\n",
    "# X_test = [X_test['open_ended_' + str(idx)].values for idx in range(1, 6)]\n",
    "\n",
    "\n",
    "# X_dev = [df_test['open_ended_' + str(idx)].values for idx in range(1, 6)]\n",
    "# X_dev_ = [df_dev['open_ended_' + str(idx)].values for idx in range(1, 6)]\n",
    "\n",
    "\n",
    "# X_train = np.array(X_train).T\n",
    "# X_test = np.array(X_test).T\n",
    "\n",
    "X_train = [X_train['open_ended_' + str(idx)] for idx in range(1, 6)]\n",
    "X_train_dev = [X_test['open_ended_' + str(idx)] for idx in range(1, 6)]\n",
    "X_test = [df_test['open_ended_' + str(idx)] for idx in range(1, 6)]\n",
    "X_dev = [df_dev['open_ended_' + str(idx)] for idx in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(df_test[[att + \"_Scale_score\" for att in ATTRIBUTE_LIST]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences (nested lists) to character IDs\n",
    "X_train_ids = [batch_to_ids([sentence.split() for sentence in X_train[idx]]).cuda() for idx in range(5)]\n",
    "X_train_dev_ids = [batch_to_ids([sentence.split() for sentence in X_train_dev[idx]]).cuda() for idx in range(5)]\n",
    "X_dev_ids = [batch_to_ids([sentence.split() for sentence in X_dev[idx]]).cuda() for idx in range(5)]\n",
    "X_test_ids = [batch_to_ids([sentence.split() for sentence in X_test[idx]]).cuda() for idx in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoConcatRegressionModel(nn.Module):\n",
    "    def __init__(self, elmo_model, input_dim, dense_dropout_rate=0.5, include_hidden_layer=False, hidden_layer_size=64):\n",
    "        super(ElmoConcatRegressionModel, self).__init__()\n",
    "        self.elmo = elmo_model\n",
    "        self.dropout = nn.Dropout(dense_dropout_rate)\n",
    "        self.include_hidden_layer = include_hidden_layer\n",
    "        \n",
    "        # Define layers\n",
    "        if include_hidden_layer:\n",
    "            self.hidden_layer = nn.Linear(5 * input_dim, hidden_layer_size)\n",
    "            self.output_layer = nn.Linear(hidden_layer_size, 1)\n",
    "        else:\n",
    "            self.output_layer = nn.Linear(5 * input_dim, 1)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        embeddings = []\n",
    "        \n",
    "        # Pass each input through ELMo and compute the mean embedding\n",
    "        for input_tensor in inputs:\n",
    "            elmo_output = self.elmo(input_tensor)['elmo_representations'][0]\n",
    "            mean_embedding = torch.mean(elmo_output, dim=1)  # Mean pooling\n",
    "            embeddings.append(mean_embedding)\n",
    "        \n",
    "        # Concatenate embeddings from all inputs\n",
    "        concat_embeddings = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Apply dropout and hidden layer if specified\n",
    "        x = self.dropout(concat_embeddings)\n",
    "        if self.include_hidden_layer:\n",
    "            x = F.relu(self.hidden_layer(x))\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Final output\n",
    "        output = self.output_layer(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 117, 102,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 100, 105,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 103, 106,  ..., 261, 261, 261],\n",
       "          [259,  98, 116,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 106, 111,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 103,  ..., 261, 261, 261],\n",
       "          [259, 106, 117,  ..., 261, 261, 261],\n",
       "          [259, 120, 102,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 117, 115,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]]], device='cuda:0'),\n",
       " tensor([[[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 116, 117,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 116, 117,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259,  98, 117,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 117, 115,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259,  98, 116,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]]], device='cuda:0'),\n",
       " tensor([[[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 117, 115,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 104, 112,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 111, 112,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 104, 112,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 101, 102,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 117, 115,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]]], device='cuda:0'),\n",
       " tensor([[[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 120, 115,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 101, 106,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 103, 102,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259,  98, 115,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 100, 105,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259,  98, 117,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]]], device='cuda:0'),\n",
       " tensor([[[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259,  75,  86,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 103, 106,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 103, 106,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 102, 111,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 103, 106,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]],\n",
       " \n",
       "         [[259,  74, 260,  ..., 261, 261, 261],\n",
       "          [259, 120, 112,  ..., 261, 261, 261],\n",
       "          [259, 103, 106,  ..., 261, 261, 261],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0]]], device='cuda:0')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(data_loader, data_tensor):\n",
    "    all_embeddings = []\n",
    "    for batch_indices in data_loader:\n",
    "        batch_data = data_tensor[batch_indices]  # Index tensors using Python lists for proper slicing\n",
    "        embeddings = elmo(batch_data)['elmo_representations'][0]  # Get ELMo embeddings\n",
    "        mean_embeddings = torch.mean(embeddings, dim=1).cuda()  # Mean across sequence length dimension\n",
    "        all_embeddings.append(mean_embeddings)\n",
    "    return torch.cat(all_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for attribute E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [01:20<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making test predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:29<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making val predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:27<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E - Test r: 0.015322170992865492\n",
      "\n",
      "Training for attribute A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [01:19<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making test predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:29<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making val predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:27<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A - Test r: -0.04467315286122029\n",
      "\n",
      "Training for attribute O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [01:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making test predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:29<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making val predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:27<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O - Test r: 0.0067910596279656685\n",
      "\n",
      "Training for attribute C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 51/109 [00:37<00:36,  1.58it/s]"
     ]
    }
   ],
   "source": [
    "# Training loop setup\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "for idx, att in enumerate(ATTRIBUTE_LIST):\n",
    "    print(f\"Training for attribute {att}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ElmoConcatRegressionModel(elmo, input_dim=1024, include_hidden_layer=True, hidden_layer_size=64).cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(torch.arange(X_train_ids[0].size(0))), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.arange(X_dev_ids[0].size(0))), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(torch.arange(X_test_ids[0].size(0))), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_indices in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Collect batch data for all inputs at once\n",
    "            batch_data = [X_train_ids[i][batch_indices].cuda() for i in range(5)]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(*batch_data).squeeze()\n",
    "\n",
    "            # Get the corresponding target values for this batch\n",
    "            batch_targets = targets_train[batch_indices].squeeze()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Evaluate in batches\n",
    "    model.eval()\n",
    "    preds_test = []\n",
    "    preds_val = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print('making test predictions')\n",
    "        for batch_indices in tqdm(test_loader):\n",
    "            batch_data = [X_test_ids[i][batch_indices].cuda() for i in range(5)]\n",
    "            preds_batch = model(*batch_data).squeeze()\n",
    "            preds_test.append(preds_batch.cpu().numpy())\n",
    "\n",
    "        print('making val predictions')\n",
    "        for batch_indices in tqdm(val_loader):\n",
    "            batch_data = [X_dev_ids[i][batch_indices].cuda() for i in range(5)]\n",
    "            preds_batch = model(*batch_data).squeeze()\n",
    "            preds_val.append(preds_batch.cpu().numpy())\n",
    "\n",
    "    # Combine predictions for final evaluation\n",
    "    preds_test = np.concatenate(preds_test)\n",
    "    preds_val = np.concatenate(preds_val)\n",
    "\n",
    "    # Save predictions\n",
    "    df_test[f'{att}_Pred'] = preds_test\n",
    "    df_dev[f'{att}_Pred'] = preds_val\n",
    "\n",
    "    pearson_r_test = pearsonr(y_test[:, idx], preds_test)[0]\n",
    "    test_scores.append(pearson_r_test)\n",
    "\n",
    "    print(f\"{att} - Test r: {pearson_r_test}\")\n",
    "    print(\"\")\n",
    "\n",
    "print(f\"Average Test r: {np.mean(test_scores)}\")\n",
    "print(f\"Average Train r: {np.mean(train_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 5)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Respondent_ID</th>\n",
       "      <th>open_ended_1</th>\n",
       "      <th>open_ended_2</th>\n",
       "      <th>open_ended_3</th>\n",
       "      <th>open_ended_4</th>\n",
       "      <th>open_ended_5</th>\n",
       "      <th>E_Scale_score</th>\n",
       "      <th>A_Scale_score</th>\n",
       "      <th>O_Scale_score</th>\n",
       "      <th>C_Scale_score</th>\n",
       "      <th>N_Scale_score</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10440136230</td>\n",
       "      <td>I would re-schedule my vacation time because I...</td>\n",
       "      <td>I would start on the project as soon as possib...</td>\n",
       "      <td>I would go by myself or convince my colleague ...</td>\n",
       "      <td>I do not feel good about the situation. I woul...</td>\n",
       "      <td>I would find this experience enjoyable. I like...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10459740644</td>\n",
       "      <td>I would likely complain privately to someone o...</td>\n",
       "      <td>I would start working on my project during all...</td>\n",
       "      <td>I want to make sure that my personal life is n...</td>\n",
       "      <td>I would be very upset particularly if the cons...</td>\n",
       "      <td>I would be very interested in learning about N...</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10446110785</td>\n",
       "      <td>I would most likely be willing to change. I am...</td>\n",
       "      <td>I would start immediately. I am not a procrast...</td>\n",
       "      <td>I would take the time and go. I would stay for...</td>\n",
       "      <td>I would feel scared, anxious, frustrated and a...</td>\n",
       "      <td>I would find this experience enjoyable. I love...</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>3.583333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10446118106</td>\n",
       "      <td>I would give my partner the vacation week. I w...</td>\n",
       "      <td>I will tell my boss in a gentle yet firm way t...</td>\n",
       "      <td>I would understand my client and not force the...</td>\n",
       "      <td>I would ask my boss to discuss this important ...</td>\n",
       "      <td>I would find this enjoyable. Norway is one of ...</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10460409624</td>\n",
       "      <td>It would depend on the plans made for my trip....</td>\n",
       "      <td>I would attempt to finish the project before t...</td>\n",
       "      <td>If I am looking to advance in my career, then ...</td>\n",
       "      <td>I would not be happy. I would listen to my man...</td>\n",
       "      <td>I would not volunteer to get involved on the p...</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>1.916667</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>10446128741</td>\n",
       "      <td>I would first try to talk and negotiate with m...</td>\n",
       "      <td>I would work on that project by breaking it up...</td>\n",
       "      <td>I would go to the meeting. That point of netwo...</td>\n",
       "      <td>I would feel upset which would motivate me to ...</td>\n",
       "      <td>I would find it enjoyable. Any chance to learn...</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>10459975899</td>\n",
       "      <td>I would probably be ok with this. Generally, I...</td>\n",
       "      <td>I would have no problem with project completio...</td>\n",
       "      <td>I would see if I could 'lean' him in the other...</td>\n",
       "      <td>I would push him for details. Who, what,... Th...</td>\n",
       "      <td>I would enjoy the experience. I've lived and w...</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>10459749226</td>\n",
       "      <td>I would first have a conversation directly wit...</td>\n",
       "      <td>I would chart out my plan to be sure I had eno...</td>\n",
       "      <td>I would probably not want to go either.  Howev...</td>\n",
       "      <td>I would take some time to think it over.  When...</td>\n",
       "      <td>I think it would be interesting.  I am Norwegi...</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>10459961050</td>\n",
       "      <td>I would look for an alternative solution.  Wor...</td>\n",
       "      <td>I would get the project done with the time tha...</td>\n",
       "      <td>I would go to the party.  Even if I do not kno...</td>\n",
       "      <td>I would ask for additional information.  I wou...</td>\n",
       "      <td>I would find it enjoyable.  Learning new thing...</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>10440135795</td>\n",
       "      <td>I would change my vacation plans. I want to av...</td>\n",
       "      <td>I would make every effort to complete the proj...</td>\n",
       "      <td>I would go to the networking meeting. Furtherm...</td>\n",
       "      <td>I would accept the feedback as given. The perc...</td>\n",
       "      <td>I would be delighted to learn more about Norwa...</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Respondent_ID                                       open_ended_1  \\\n",
       "0      10440136230  I would re-schedule my vacation time because I...   \n",
       "1      10459740644  I would likely complain privately to someone o...   \n",
       "2      10446110785  I would most likely be willing to change. I am...   \n",
       "3      10446118106  I would give my partner the vacation week. I w...   \n",
       "4      10460409624  It would depend on the plans made for my trip....   \n",
       "..             ...                                                ...   \n",
       "295    10446128741  I would first try to talk and negotiate with m...   \n",
       "296    10459975899  I would probably be ok with this. Generally, I...   \n",
       "297    10459749226  I would first have a conversation directly wit...   \n",
       "298    10459961050  I would look for an alternative solution.  Wor...   \n",
       "299    10440135795  I would change my vacation plans. I want to av...   \n",
       "\n",
       "                                          open_ended_2  \\\n",
       "0    I would start on the project as soon as possib...   \n",
       "1    I would start working on my project during all...   \n",
       "2    I would start immediately. I am not a procrast...   \n",
       "3    I will tell my boss in a gentle yet firm way t...   \n",
       "4    I would attempt to finish the project before t...   \n",
       "..                                                 ...   \n",
       "295  I would work on that project by breaking it up...   \n",
       "296  I would have no problem with project completio...   \n",
       "297  I would chart out my plan to be sure I had eno...   \n",
       "298  I would get the project done with the time tha...   \n",
       "299  I would make every effort to complete the proj...   \n",
       "\n",
       "                                          open_ended_3  \\\n",
       "0    I would go by myself or convince my colleague ...   \n",
       "1    I want to make sure that my personal life is n...   \n",
       "2    I would take the time and go. I would stay for...   \n",
       "3    I would understand my client and not force the...   \n",
       "4    If I am looking to advance in my career, then ...   \n",
       "..                                                 ...   \n",
       "295  I would go to the meeting. That point of netwo...   \n",
       "296  I would see if I could 'lean' him in the other...   \n",
       "297  I would probably not want to go either.  Howev...   \n",
       "298  I would go to the party.  Even if I do not kno...   \n",
       "299  I would go to the networking meeting. Furtherm...   \n",
       "\n",
       "                                          open_ended_4  \\\n",
       "0    I do not feel good about the situation. I woul...   \n",
       "1    I would be very upset particularly if the cons...   \n",
       "2    I would feel scared, anxious, frustrated and a...   \n",
       "3    I would ask my boss to discuss this important ...   \n",
       "4    I would not be happy. I would listen to my man...   \n",
       "..                                                 ...   \n",
       "295  I would feel upset which would motivate me to ...   \n",
       "296  I would push him for details. Who, what,... Th...   \n",
       "297  I would take some time to think it over.  When...   \n",
       "298  I would ask for additional information.  I wou...   \n",
       "299  I would accept the feedback as given. The perc...   \n",
       "\n",
       "                                          open_ended_5  E_Scale_score  \\\n",
       "0    I would find this experience enjoyable. I like...       3.000000   \n",
       "1    I would be very interested in learning about N...       2.916667   \n",
       "2    I would find this experience enjoyable. I love...       3.333333   \n",
       "3    I would find this enjoyable. Norway is one of ...       2.833333   \n",
       "4    I would not volunteer to get involved on the p...       2.916667   \n",
       "..                                                 ...            ...   \n",
       "295  I would find it enjoyable. Any chance to learn...       4.166667   \n",
       "296  I would enjoy the experience. I've lived and w...       4.250000   \n",
       "297  I think it would be interesting.  I am Norwegi...       2.416667   \n",
       "298  I would find it enjoyable.  Learning new thing...       3.750000   \n",
       "299  I would be delighted to learn more about Norwa...       4.166667   \n",
       "\n",
       "     A_Scale_score  O_Scale_score  C_Scale_score  N_Scale_score Dataset  \n",
       "0         4.333333       4.166667       4.166667       2.333333    Test  \n",
       "1         3.500000       3.750000       4.250000       1.666667    Test  \n",
       "2         4.166667       3.583333       5.000000       1.500000    Test  \n",
       "3         4.000000       4.000000       4.500000       1.666667    Test  \n",
       "4         4.583333       3.500000       4.583333       1.916667    Test  \n",
       "..             ...            ...            ...            ...     ...  \n",
       "295       4.583333       3.250000       5.000000       1.333333    Test  \n",
       "296       3.750000       4.166667       4.916667       1.750000    Test  \n",
       "297       4.166667       3.416667       4.833333       2.333333    Test  \n",
       "298       3.500000       4.333333       4.333333       3.083333    Test  \n",
       "299       4.416667       4.000000       5.000000       1.166667    Test  \n",
       "\n",
       "[300 rows x 12 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\n",
    "    \"preds_test_01.csv\",\n",
    "    columns=[\"Respondent_ID\", *[sym + \"_Pred\" for sym in ATTRIBUTE_LIST]],\n",
    "    index=False\n",
    ")\n",
    "\n",
    "df_dev.to_csv(\n",
    "    \"preds_dev_01.csv\",\n",
    "    columns=[\"Respondent_ID\", *[sym + \"_Pred\" for sym in ATTRIBUTE_LIST]],\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winning submission\n",
    "\n",
    "Each of the three sets of predicted values generated from the above code were submitted to the private leader board. With the exception of Openness, the best predictors from those were then averaged together to form a fourth submission. Our openness predictor was poor, so we continued to tinker with it on the fourth submission. In all cases the averaged values had stronger correlations than the independent values.\n",
    "\n",
    "The final submission was as follows:\n",
    "- Openness: Word List\n",
    "- Concientiousness: averaged the z-transformed predicted values from World List and Deep Learning\n",
    "- Agreeableness: averaged the z-transformed predicted values from Machine Learning and Deep Learning\n",
    "- Extraversion: averaged the z-transformed predicted values from Word List and Deep Learning\n",
    "- Neuroticism: averaged the z-transformed predicted values from Word List and Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
